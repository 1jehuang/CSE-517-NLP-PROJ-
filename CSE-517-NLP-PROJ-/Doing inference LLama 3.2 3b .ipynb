{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"e323c95b48534d5ba19edc963c24362b":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_05124fdc39e8484098dd04ed2342f643","IPY_MODEL_dd3ed58df5ea4d608a0b0fc1244bf6cc","IPY_MODEL_c6dd30e096354a72a5dfd1c8136abec5"],"layout":"IPY_MODEL_aff78e2f419748a59cc0470f592fa446"}},"05124fdc39e8484098dd04ed2342f643":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_323e6a7f975b421492da0fbb3f42e122","placeholder":"​","style":"IPY_MODEL_b47760c4b02f4cdb8c8b5dc325c68e3e","value":"Loading checkpoint shards:  50%"}},"dd3ed58df5ea4d608a0b0fc1244bf6cc":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"","description":"","description_tooltip":null,"layout":"IPY_MODEL_c7b6016e70ac4640a28b0eb01e071169","max":2,"min":0,"orientation":"horizontal","style":"IPY_MODEL_e5bda379ea3b40e2a5654b74ca48e10f","value":1}},"c6dd30e096354a72a5dfd1c8136abec5":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_8fb9633a8d8d4d4db2a8d5b30244b5c4","placeholder":"​","style":"IPY_MODEL_87f6405239de4b7c9c6c3859a386c683","value":" 1/2 [00:23&lt;00:23, 23.25s/it]"}},"aff78e2f419748a59cc0470f592fa446":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"323e6a7f975b421492da0fbb3f42e122":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b47760c4b02f4cdb8c8b5dc325c68e3e":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"c7b6016e70ac4640a28b0eb01e071169":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e5bda379ea3b40e2a5654b74ca48e10f":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"8fb9633a8d8d4d4db2a8d5b30244b5c4":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"87f6405239de4b7c9c6c3859a386c683":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"cells":[{"cell_type":"code","source":["# adws# prompt: load drive\n","\n","from google.colab import drive\n","drive.mount('/content/drive')\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1oxU18WKUlSN","executionInfo":{"status":"ok","timestamp":1738569013296,"user_tz":480,"elapsed":19541,"user":{"displayName":"Jeremy","userId":"17694110375770310806"}},"outputId":"8dbfce55-fdf4-44a1-f518-b7217d9da97d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":49,"referenced_widgets":["e323c95b48534d5ba19edc963c24362b","05124fdc39e8484098dd04ed2342f643","dd3ed58df5ea4d608a0b0fc1244bf6cc","c6dd30e096354a72a5dfd1c8136abec5","aff78e2f419748a59cc0470f592fa446","323e6a7f975b421492da0fbb3f42e122","b47760c4b02f4cdb8c8b5dc325c68e3e","c7b6016e70ac4640a28b0eb01e071169","e5bda379ea3b40e2a5654b74ca48e10f","8fb9633a8d8d4d4db2a8d5b30244b5c4","87f6405239de4b7c9c6c3859a386c683"]},"id":"nJIaF8QxT3ol","outputId":"aa663568-aa46-45c3-dfc1-9ded57f9abf4"},"outputs":[{"output_type":"display_data","data":{"text/plain":["Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e323c95b48534d5ba19edc963c24362b"}},"metadata":{}}],"source":["from transformers import AutoModelForCausalLM, AutoTokenizer\n","\n","# Define the directory where the model is saved\n","load_directory = \"/content/drive/Shareddrives/517 nlp project/Models/Llama-3.2-3B-Instruct\"\n","\n","# Load the model and tokenizer from the specified directory\n","tokenizer = AutoTokenizer.from_pretrained(load_directory)\n","model = AutoModelForCausalLM.from_pretrained(load_directory)\n","\n","print(\"Model and tokenizer successfully loaded from Google Drive!\")\n"]},{"cell_type":"code","source":["import time\n","from transformers import AutoTokenizer, AutoModelForCausalLM\n","\n","# Load the model and tokenizer\n","load_directory = \"/content/drive/Shareddrives/517 nlp project/Models/Llama-3.2-3B-Instruct\"\n","tokenizer = AutoTokenizer.from_pretrained(load_directory)\n","model = AutoModelForCausalLM.from_pretrained(load_directory).to(\"cuda\")\n","\n","# Ensure the tokenizer has a pad_token\n","if tokenizer.pad_token is None:\n","    tokenizer.add_special_tokens({'pad_token': tokenizer.eos_token})\n","    model.resize_token_embeddings(len(tokenizer))  # Resize model's embedding matrix if a new token is added\n","\n","# Prompt the model to generate the longest possible response\n","prompt = (\n","    \"debate relgiion and atheism to completition\"\n",")\n","\n","# Tokenize the input\n","inputs = tokenizer(prompt, return_tensors=\"pt\", padding=True).to(\"cuda\")\n","\n","# Measure inference time\n","start_time = time.time()\n","\n","# Generate output with maximum length\n","output = model.generate(\n","    inputs.input_ids,\n","    max_length=2048,  # Maximum length for generation\n","    temperature=0.8,  # Controls randomness; lower is more deterministic\n","    top_p=0.9,        # Controls diversity\n","    repetition_penalty=1.1,  # Penalizes repetitive output\n","    do_sample=True,   # Enables sampling\n","    use_cache=True,   # Optimizes performance by caching computations\n","    pad_token_id=tokenizer.pad_token_id  # Explicitly set pad_token_id\n",")\n","\n","# Measure the time taken\n","end_time = time.time()\n","\n","# Decode the output\n","generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n","\n","# Calculate metrics\n","elapsed_time = end_time - start_time\n","tokens_generated = len(generated_text.split())  # Approximate number of tokens\n","tokens_per_second = tokens_generated / elapsed_time\n","\n","# Print the response\n","print(\"--- Generated Response ---\")\n","print(generated_text)\n","\n","# Print performance statistics\n","print(\"\\n--- Performance Statistics ---\")\n","print(f\"Elapsed Time: {elapsed_time:.2f} seconds\")\n","print(f\"Tokens Generated: {tokens_generated}\")\n","print(f\"Tokens per Second: {tokens_per_second:.2f}\")\n","print(f\"Prompt Length (Tokens): {inputs.input_ids.shape[-1]}\")\n","print(f\"Generated Length (Tokens): {tokens_generated}\")\n"],"metadata":{"id":"-dXBV9MXW_5K"},"execution_count":null,"outputs":[]},{"source":["import re\n","\n","def extract_equation(text):\n","    \"\"\"Extracts the equation from the generated text using regular expressions.\"\"\"\n","    match = re.search(r\"[-+*/()]?\\d+\\s*[-+*/()]\\s*\\d+\", text)  # Find simple arithmetic operations\n","    if match:\n","        return match.group(0)  # Return the matched equation\n","    else:\n","        return \"\"  # Return empty string if no equation is found"],"cell_type":"code","metadata":{"id":"tYzMxQHVfJBn"},"execution_count":null,"outputs":[]},{"source":["import pandas as pd\n","from sklearn.metrics import accuracy_score"],"cell_type":"code","metadata":{"id":"xRI2blisezdg"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":[],"metadata":{"id":"473tIJDOlKlf"}},{"source":["import pandas as pd\n","\n","# Load dataset\n","dataset_path = \"/content/drive/Shareddrives/517 nlp project/data/SVAMP/data/mawps-asdiv-a_svamp_without_questions/dev.csv\"\n","df = pd.read_csv(dataset_path)\n","\n","# Define formatting function - now without showing the answer\n","def format_question(row):\n","    # Get the numbers from the Numbers column\n","    numbers = row['Numbers'].split()\n","\n","    # Replace number0, number1, etc with actual values\n","    body = row['Body']\n","    question = row['Ques']\n","\n","    for i, num in enumerate(numbers):\n","        body = body.replace(f'number{i}', num)\n","        question = question.replace(f'number{i}', num)\n","\n","    return f\"Problem: {body}\\nQuestion: {question}\"\n","\n","# Create formatted question column with actual numbers\n","df['formatted_question'] = df.apply(format_question, axis=1)\n","\n","# Display first 3 examples with clear formatting\n","print(\"Sample formatted questions:\\n\")\n","for idx, row in df.head(3).iterrows():\n","    print(f\"Example {idx + 1}:\")\n","    print(f\"{row['formatted_question']}\")\n","    print(f\"Answer (not shown to model): {row['Answer']}\")\n","    print(\"---\\n\")"],"cell_type":"code","metadata":{"id":"7UVIzJzfe0BH"},"execution_count":null,"outputs":[]},{"source":["def generate_math_prompt(question):\n","    prompt = f\"\"\"Solve this math word problem step by step:\n","\n","{question}\n","\n","Let's solve this step by step:\n","1) First, understand what is being asked\n","2) Then, identify the important information\n","3) Solve step by step\n","4) Finally, state the answer in the format: [ANSWER]<number>[/ANSWER]\n","\n","Show your work:\"\"\"\n","    return prompt\n","\n","def extract_answer(response):\n","    \"\"\"Extract the numerical answer from between [ANSWER] tags\"\"\"\n","    try:\n","        # Look for answer between tags\n","        import re\n","        pattern = r'\\[ANSWER\\](\\d+\\.?\\d*)\\[\\/ANSWER\\]'\n","        match = re.search(pattern, response)\n","        if match:\n","            return float(match.group(1))\n","\n","        # Fallback: look for last number after \"answer is\" or similar phrases\n","        patterns = [\n","            r'answer is[^\\d]*(\\d+\\.?\\d*)',\n","            r'answer:[^\\d]*(\\d+\\.?\\d*)',\n","            r'equals[^\\d]*(\\d+\\.?\\d*)',\n","            r'=\\s*(\\d+\\.?\\d*)'\n","        ]\n","\n","        for pattern in patterns:\n","            match = re.search(pattern, response.lower())\n","            if match:\n","                return float(match.group(1))\n","\n","        # Final fallback: last number in response\n","        numbers = re.findall(r'\\d+\\.?\\d*', response)\n","        if numbers:\n","            return float(numbers[-1])\n","\n","        return None\n","    except:\n","        return None"],"cell_type":"code","metadata":{"id":"iDRsrdaWe0Jo"},"execution_count":null,"outputs":[]},{"source":["# Run inference on test set\n","predictions = []\n","actual = []\n","start_time = time.time()\n","\n","# ANSI color codes\n","BLUE = '\\033[94m'\n","GREEN = '\\033[92m'\n","RED = '\\033[91m'\n","YELLOW = '\\033[93m'\n","CYAN = '\\033[96m'\n","PURPLE = '\\033[95m'  # New color for model response\n","ENDC = '\\033[0m'\n","\n","# Take first 10 examples for testing\n","total_questions = len(df.head(10))\n","\n","for idx, row in df.head(10).iterrows():\n","    question = row['formatted_question']\n","    prompt = generate_math_prompt(question)\n","\n","    # Generate response\n","    inputs = tokenizer(prompt, return_tensors=\"pt\", padding=True).to(\"cuda\")\n","    output = model.generate(\n","        inputs.input_ids,\n","        max_length=512,\n","        temperature=0.1,  # Lower temperature for more focused answers\n","        do_sample=True,\n","        num_beams=3,      # Add beam search for better coherence\n","        top_p=0.9,        # Nucleus sampling\n","        repetition_penalty=1.2  # Increased penalty for repetition\n","    )\n","    response = tokenizer.decode(output[0], skip_special_tokens=True)\n","\n","    # Extract numerical answer\n","    predicted = extract_answer(response)\n","    if predicted is not None:\n","        predictions.append(predicted)\n","        actual.append(float(row['Answer']))\n","\n","        is_correct = abs(predicted - float(row['Answer'])) < 0.01\n","        correct_color = GREEN if is_correct else RED\n","\n","        # Print progress and time estimation\n","        questions_done = len(predictions)\n","        questions_left = total_questions - questions_done\n","        elapsed_time = time.time() - start_time\n","        avg_time_per_question = elapsed_time / questions_done if questions_done > 0 else 0\n","        estimated_time_left = questions_left * avg_time_per_question\n","\n","        print(f\"\\n{CYAN}Progress: {questions_done}/{total_questions} questions{ENDC}\")\n","        print(f\"{CYAN}Average time per question: {avg_time_per_question:.1f} seconds{ENDC}\")\n","        print(f\"{CYAN}Estimated time remaining: {estimated_time_left/60:.1f} minutes{ENDC}\\n\")\n","\n","        # Print results\n","        print(f\"\\n{YELLOW}Question {idx}:{ENDC}\")\n","        print(f\"{BLUE}Prompt: {question}{ENDC}\")\n","        print(f\"{YELLOW}Model response:{ENDC}\")\n","        print(f\"{PURPLE}{response}{ENDC}\")  # Color the entire response\n","        print(f\"{correct_color}Correct: {is_correct}{ENDC}\")"],"cell_type":"code","metadata":{"id":"F2v0qsq4e0o5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Calculate accuracy\n","correct = 0\n","print(f\"Mean absolute error: {errors.mean():.2f}\")\n","print(f\"Median absolute error: {np.median(errors):.2f}\")"],"metadata":{"id":"QRDtZBc8qyPE"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["NOTE: FOR THE SVAMP DATASET, there are multiple folders. there are three datasets, easy math, med math, and harder math. for two of the datasets, they have a question version and a no question version. i find they are identicle. they also have a combined (all three) dataset (there is one labeled questions and one labeled no questions). just use one of the combined datasets"],"metadata":{"id":"wZHA-DYuKTaS"}}]}
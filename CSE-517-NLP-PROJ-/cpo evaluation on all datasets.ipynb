{"cells":[{"cell_type":"markdown","metadata":{"id":"zl1EikjgyHwi"},"source":["# Evaluating Llama-3.2-1B on Multiple Datasets\n","\n","This notebook evaluates the Llama-3.2-1B model on multiple datasets:\n","- **VitaminC**: Fact verification\n","- **FEVER/FEVEROUS**: Fact verification\n","- **HotpotQA/2WikiMultihopQA**: Multi-hop question answering\n","- **SVAMP**: Math word problems\n","- **Bamboogle**: General question answering\n","\n","The evaluation uses dataset-specific prompting strategies and metrics."]},{"cell_type":"code","execution_count":28,"metadata":{"id":"_rseoOajyHwi","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1741708567918,"user_tz":420,"elapsed":545,"user":{"displayName":"Jeremy Huang","userId":"00765697131879146443"}},"outputId":"2aa23b1d-7d0d-4fb0-adae-18ce8834d9b2"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["import json\n","import time\n","import re\n","import random\n","import os\n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","from google.colab import drive\n","from transformers import AutoModelForCausalLM, AutoTokenizer\n","from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n","import torch\n","import math\n","\n","# ANSI color codes for terminal output\n","BLUE = '\\033[94m'     # For sample information\n","GREEN = '\\033[92m'    # For correct predictions and success messages\n","RED = '\\033[91m'      # For incorrect predictions\n","YELLOW = '\\033[93m'   # For predictions and headers\n","CYAN = '\\033[96m'     # For progress information\n","PURPLE = '\\033[95m'   # For model responses\n","BOLD = '\\033[1m'      # Bold text\n","ENDC = '\\033[0m'      # End color\n","\n","# Mount Google Drive\n","drive.mount('/content/drive')"]},{"cell_type":"markdown","metadata":{"id":"EkF1m0HgyHwj"},"source":["## Dataset Loading Functions\n","\n","Functions to load different dataset formats (JSON and JSONL)"]},{"cell_type":"code","execution_count":29,"metadata":{"id":"2X_wwVyoyHwj","executionInfo":{"status":"ok","timestamp":1741708567984,"user_tz":420,"elapsed":66,"user":{"displayName":"Jeremy Huang","userId":"00765697131879146443"}}},"outputs":[],"source":["def load_dataset(file_path, dataset_name, limit=150):\n","    \"\"\"Load samples from various datasets with appropriate format handling\"\"\"\n","    print(f\"{CYAN}Loading {dataset_name} dataset from {file_path}...{ENDC}\")\n","    data = []\n","\n","    if not os.path.exists(file_path):\n","        print(f\"{RED}File not found: {file_path}{ENDC}\")\n","        return []\n","\n","    if file_path.endswith('.jsonl'):\n","        with open(file_path, 'r', encoding='utf-8') as f:\n","            for line in f:\n","                if line.strip():  # Skip empty lines\n","                    data.append(json.loads(line))\n","                    if len(data) >= limit:\n","                        break\n","    else:  # .json files\n","        with open(file_path, 'r', encoding='utf-8') as f:\n","            json_data = json.load(f)\n","            if isinstance(json_data, list):\n","                data = json_data[:limit]\n","            else:\n","                # Handle nested structures if needed\n","                if 'data' in json_data:\n","                    data = json_data['data'][:limit]\n","                else:\n","                    print(f\"{YELLOW}Warning: Unexpected JSON structure in {file_path}{ENDC}\")\n","                    data = [json_data]  # Just use the whole object as one sample\n","\n","    print(f\"{CYAN}Loaded {len(data)} samples from {dataset_name}.{ENDC}\")\n","    return data"]},{"cell_type":"markdown","metadata":{"id":"9vaoNO3IyHwj"},"source":["## Prompt Creation Functions\n","\n","Different prompting strategies for each dataset type"]},{"cell_type":"code","execution_count":30,"metadata":{"id":"9VGJ7wQmyHwk","executionInfo":{"status":"ok","timestamp":1741708567985,"user_tz":420,"elapsed":2,"user":{"displayName":"Jeremy Huang","userId":"00765697131879146443"}}},"outputs":[],"source":["def create_prompt(sample, dataset_name):\n","    \"\"\"Create appropriate prompts based on dataset type\"\"\"\n","\n","    if dataset_name == \"VitaminC\" or dataset_name == \"FEVER\" or dataset_name == \"FEVEROUS\":\n","        # Fact verification datasets\n","        if dataset_name == \"VitaminC\":\n","            claim = sample[\"claim\"]\n","            evidence = sample[\"evidence\"]\n","        elif dataset_name == \"FEVER\":\n","            claim = sample[\"claim\"]\n","            evidence = sample.get(\"evidence\", \"\") or sample.get(\"context\", \"\")\n","        elif dataset_name == \"FEVEROUS\":\n","            claim = sample[\"claim\"]\n","            evidence = sample.get(\"evidence\", \"\") or sample.get(\"context\", \"\")\n","\n","        return create_fact_verification_prompt(claim, evidence)\n","\n","    elif dataset_name == \"HotpotQA\" or dataset_name == \"2WikiMultihopQA\":\n","        # Multi-hop QA datasets\n","        question = sample.get(\"question\", \"\") or sample.get(\"query\", \"\")\n","        context = sample.get(\"context\", \"\")\n","        if not context and \"original_context\" in sample:\n","            context = sample[\"original_context\"]\n","\n","        return create_qa_prompt(question, context)\n","\n","    elif dataset_name == \"SVAMP\":\n","        # Math word problem dataset\n","        question = sample.get(\"question\", \"\") or sample.get(\"body\", \"\")\n","        return create_math_prompt(question)\n","\n","    elif dataset_name == \"Bamboogle\":\n","        # Bamboogle dataset - assume it's a QA task\n","        question = sample.get(\"question\", \"\")\n","        context = sample.get(\"context\", \"\")\n","\n","        if not question and \"answer\" in sample:\n","            # If no question is provided but there's an answer, create a generic prompt\n","            return\n","\n","        return create_qa_prompt(question, context)\n","\n","    else:\n","        # Generic prompt for unknown datasets\n","        return f\"Please analyze this data and think step-by-step:\\n\\n{json.dumps(sample, indent=2)}\\n\\nAfter your thinking, provide your answer in a latex boxed format:\\n$\\\\boxed{{<finalAnswer>}}$\"\n","\n","def create_fact_verification_prompt(claim, evidence):\n","    \"\"\"Create a prompt for fact checking\"\"\"\n","    prompt = f\"\"\"Claim: {claim}\n","\n","Evidence: {evidence}\n","\n","Think step-by-step to determine if the evidence SUPPORTS, REFUTES, or provides NOT ENOUGH INFO for the claim.\n","\n","After your thinking, end your answer with one of these in a latex boxed format:\n","$\\\\boxed{{SUPPORTS}}$ or $\\\\boxed{{REFUTES}}$ or $\\\\boxed{{NOT ENOUGH INFO}}$\n","\n","Always end with your answer in a latex boxed format:\n","$\\\\boxed{{<finalAnswer>}}$\"\"\"\n","    return prompt\n","\n","def create_qa_prompt(question, context):\n","    \"\"\"Create a prompt for question answering tasks\"\"\"\n","    prompt = f\"\"\"\n","\n","Question: {question}\n","\n","Think step-by-step to answer the question based on the context.\n","\n","After your thinking, provide your final answer in a latex boxed format:\n","$\\\\boxed{{<finalAnswer>}}$\n","\"\"\"\n","    return prompt\n","\n","def create_math_prompt(question):\n","    \"\"\"Create a prompt for math word problems\"\"\"\n","    prompt = f\"\"\"Problem: {question}\n","\n","Think step-by-step to solve this math problem.\n","\n","After your thinking, provide your final numeric answer in a latex boxed format:\n","$\\\\boxed{{<finalAnswer>}}$\n","\"\"\"\n","    return prompt\n"]},{"cell_type":"code","source":["print( create_math_prompt(\"hi\"))"],"metadata":{"id":"gfX_gtYZZ6Da","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1741708567990,"user_tz":420,"elapsed":4,"user":{"displayName":"Jeremy Huang","userId":"00765697131879146443"}},"outputId":"5f224c0f-5cf0-4f36-a310-a23a018bc6aa"},"execution_count":31,"outputs":[{"output_type":"stream","name":"stdout","text":["Problem: hi\n","\n","Think step-by-step to solve this math problem.\n","\n","After your thinking, provide your final numeric answer in a latex boxed format:\n","$\\boxed{<finalAnswer>}$\n","\n"]}]},{"cell_type":"markdown","metadata":{"id":"IMgkydTGyHwk"},"source":["## Model Inference Function"]},{"cell_type":"code","execution_count":32,"metadata":{"id":"aYwi1s50yHwl","executionInfo":{"status":"ok","timestamp":1741708567997,"user_tz":420,"elapsed":7,"user":{"displayName":"Jeremy Huang","userId":"00765697131879146443"}}},"outputs":[],"source":["\n","def run_inference(model, tokenizer, prompt, device=\"cuda\"):\n","    \"\"\"Generate response using the chat template format\"\"\"\n","    # Create a messages array with user prompt\n","    messages = [\n","        {\"role\": \"user\", \"content\": prompt},\n","    ]\n","\n","    # Apply the chat template\n","    tokenized_chat = tokenizer.apply_chat_template(\n","        messages,\n","        tokenize=True,\n","        add_generation_prompt=True,\n","        return_tensors=\"pt\"\n","    ).to(device)\n","\n","    # Generate response\n","    with torch.no_grad():\n","        outputs = model.generate(\n","            input_ids=tokenized_chat,\n","            max_new_tokens=2048,\n","            temperature=0.2,\n","            do_sample=True,\n","        )\n","\n","    # Decode the response\n","    full_response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n","\n","    # Extract just the assistant's response by removing the prompt\n","    # This depends on how the specific model formats its responses\n","    # You might need to adjust this based on the model's output format\n","    response_parts = full_response.split(\"Assistant: \")\n","    if len(response_parts) > 1:\n","        return response_parts[-1].strip()  # Get the assistant's response\n","    else:\n","        return full_response  # Return full response if we can't split\n","\n","    return response"]},{"cell_type":"markdown","metadata":{"id":"ARcR_SomyHwl"},"source":["## Prediction Extraction Functions\n","\n","Functions to extract structured predictions from model outputs"]},{"cell_type":"code","execution_count":33,"metadata":{"id":"gp0jtmxWyHwl","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1741708568006,"user_tz":420,"elapsed":7,"user":{"displayName":"Jeremy Huang","userId":"00765697131879146443"}},"outputId":"19075685-b95b-4f09-e1fa-60518b5ae4db"},"outputs":[{"output_type":"stream","name":"stdout","text":["Test: Fraction inside box - expecting \\frac{1}{2}\n","  Dataset: RandomDataset\n","  Output: No recognized dataset... final is $\\boxed{\\frac{1}{2}}$\n","  Expected: '\\\\\\\\frac{1}{2}', Got: '\\\\frac{1'\n","  => FAILED\n","\n","Test: No box at all (should return None)\n","  Dataset: VitaminC\n","  Output: This text does not contain any \\boxed{} expression.\n","  Expected: None, Got: None\n","  => PASSED\n","\n","Test: Box with empty content (should return None)\n","  Dataset: SVAMP\n","  Output: Check \\boxed{} here. That's all.\n","  Expected: None, Got: None\n","  => PASSED\n","\n","Test: Multiple boxes, last is fraction\n","  Dataset: HotpotQA\n","  Output: Mid: $\\boxed{SUPPORTS}$ Then final: $\\boxed{\\frac{2}{5}}$\n","  Expected: '\\\\\\\\frac{2}{5}', Got: '\\\\frac{2'\n","  => FAILED\n","\n","Passed 2/4 tests.\n"]}],"source":["import re\n","\n","def extract_prediction(output, dataset_name, sample=None):\n","    \"\"\"\n","    Extract the final boxed answer for any dataset.\n","    If we only find an empty \\boxed{}, treat it as no valid box.\n","    \"\"\"\n","    # For now, ignoring dataset_name / sample, just returning the last \\boxed{...} if not empty\n","    pattern = r'\\\\boxed\\s*\\{([^}]*)\\}'\n","    all_matches = re.findall(pattern, output)\n","\n","    # Filter out empty or whitespace-only matches\n","    non_empty_matches = [m.strip() for m in all_matches if m.strip()]\n","\n","    # If no valid non-empty box found, return None\n","    if not non_empty_matches:\n","        return None\n","\n","    # Return the last valid boxed expression\n","    return non_empty_matches[-1]\n","\n","\n","def run_tests():\n","    test_cases = [\n","        {\n","            \"name\": \"Fraction inside box - expecting \\\\frac{1}{2}\",\n","            \"dataset\": \"RandomDataset\",\n","            \"output\": r\"No recognized dataset... final is $\\boxed{\\frac{1}{2}}$\",\n","            \"expected\": r\"\\\\frac{1}{2}\"\n","        },\n","        {\n","            \"name\": \"No box at all (should return None)\",\n","            \"dataset\": \"VitaminC\",\n","            \"output\": r\"This text does not contain any \\boxed{} expression.\",\n","            \"expected\": None\n","        },\n","        {\n","            \"name\": \"Box with empty content (should return None)\",\n","            \"dataset\": \"SVAMP\",\n","            \"output\": r\"Check \\boxed{} here. That's all.\",\n","            \"expected\": None\n","        },\n","        {\n","            \"name\": \"Multiple boxes, last is fraction\",\n","            \"dataset\": \"HotpotQA\",\n","            \"output\": (\n","                r\"Mid: $\\boxed{SUPPORTS}$ \"\n","                r\"Then final: $\\boxed{\\frac{2}{5}}$\"\n","            ),\n","            \"expected\": r\"\\\\frac{2}{5}\"\n","        },\n","    ]\n","\n","    passes = 0\n","    total = len(test_cases)\n","\n","    for t in test_cases:\n","        pred = extract_prediction(t[\"output\"], t[\"dataset\"])\n","        success = (pred == t[\"expected\"])\n","        if success:\n","            passes += 1\n","\n","        print(f\"Test: {t['name']}\")\n","        print(f\"  Dataset: {t['dataset']}\")\n","        print(f\"  Output: {t['output']}\")\n","        print(f\"  Expected: {t['expected']!r}, Got: {pred!r}\")\n","        print(f\"  => {'PASSED' if success else 'FAILED'}\\n\")\n","\n","    print(f\"Passed {passes}/{total} tests.\")\n","\n","\n","if __name__ == \"__main__\":\n","    run_tests()\n"]},{"cell_type":"markdown","metadata":{"id":"iBl6VmcEyHwm"},"source":["## Evaluation Metrics Functions\n","\n","Functions to compare predictions with ground truth"]},{"cell_type":"code","execution_count":34,"metadata":{"id":"aMwHp_1JyHwm","executionInfo":{"status":"ok","timestamp":1741708568022,"user_tz":420,"elapsed":16,"user":{"displayName":"Jeremy Huang","userId":"00765697131879146443"}}},"outputs":[],"source":["def evaluate_correctness(prediction, ground_truth, dataset_name):\n","    \"\"\"Evaluate if the prediction is correct based on dataset type\"\"\"\n","\n","    if dataset_name == \"VitaminC\" or dataset_name == \"FEVER\" or dataset_name == \"FEVEROUS\":\n","        # Fact verification - direct comparison\n","        return prediction == ground_truth\n","\n","    elif dataset_name == \"HotpotQA\" or dataset_name == \"2WikiMultihopQA\" or dataset_name == \"Bamboogle\":\n","        # QA evaluation - normalize and compare\n","        return normalize_qa_answers(prediction, ground_truth)\n","\n","    elif dataset_name == \"SVAMP\":\n","        # Math evaluation - numeric comparison\n","        return evaluate_math_correctness(prediction, ground_truth)\n","\n","    else:\n","        # Generic comparison for unknown datasets\n","        return prediction == ground_truth\n","\n","def normalize_qa_answers(prediction, ground_truth):\n","    \"\"\"Normalize and compare QA answers with flexible matching\"\"\"\n","    if not prediction or not ground_truth:\n","        return False\n","\n","    # Handle list or dictionary ground truths\n","    if isinstance(ground_truth, list):\n","        ground_truth = \" \".join([str(item) for item in ground_truth])\n","    elif isinstance(ground_truth, dict):\n","        if \"answer\" in ground_truth:\n","            ground_truth = ground_truth[\"answer\"]\n","        else:\n","            ground_truth = str(ground_truth)\n","\n","    # Normalize both strings\n","    pred_norm = prediction.lower().strip()\n","    truth_norm = str(ground_truth).lower().strip()\n","\n","    # Remove punctuation and extra spaces\n","    pred_norm = re.sub(r'[^\\w\\s]', '', pred_norm).strip()\n","    truth_norm = re.sub(r'[^\\w\\s]', '', truth_norm).strip()\n","\n","    # Check if prediction contains ground truth or vice versa\n","    return pred_norm in truth_norm or truth_norm in pred_norm\n","\n","def evaluate_math_correctness(prediction, ground_truth):\n","    \"\"\"Evaluate correctness of math answers with tolerance\"\"\"\n","    try:\n","        # Convert to numeric values\n","        if isinstance(prediction, str):\n","            prediction = float(re.search(r'(-?[\\d.]+)', prediction.replace(',', '')).group(1))\n","\n","        if isinstance(ground_truth, str):\n","            ground_truth = float(re.search(r'(-?[\\d.]+)', ground_truth.replace(',', '')).group(1))\n","\n","        # Compare with tolerance\n","        tolerance = 0.01\n","        return abs(float(prediction) - float(ground_truth)) < tolerance\n","    except (ValueError, TypeError, AttributeError):\n","        return False\n","\n","def get_ground_truth(sample, dataset_name):\n","    \"\"\"Extract ground truth from sample based on dataset type\"\"\"\n","\n","    if dataset_name == \"VitaminC\":\n","        return sample.get(\"label\", \"\")\n","\n","    elif dataset_name == \"FEVER\" or dataset_name == \"FEVEROUS\":\n","        return sample.get(\"label\", \"\")\n","\n","    elif dataset_name == \"HotpotQA\" or dataset_name == \"2WikiMultihopQA\":\n","        return sample.get(\"answer\", \"\")\n","\n","    elif dataset_name == \"SVAMP\":\n","        return sample.get(\"answer\", None)\n","\n","    elif dataset_name == \"Bamboogle\":\n","        return sample.get(\"answer\", \"\")\n","\n","    else:\n","        return None"]},{"cell_type":"markdown","metadata":{"id":"XxhWQHyQyHwm"},"source":["## Visualization Functions\n","\n","Functions to create visualizations of results"]},{"cell_type":"code","execution_count":35,"metadata":{"id":"Ym19hvtvyHwm","executionInfo":{"status":"ok","timestamp":1741708568095,"user_tz":420,"elapsed":74,"user":{"displayName":"Jeremy Huang","userId":"00765697131879146443"}}},"outputs":[],"source":["def visualize_results(results_df, dataset_name):\n","    \"\"\"Create visualizations of results for a specific dataset\"\"\"\n","    output_dir = f\"results_{dataset_name}\"\n","    os.makedirs(output_dir, exist_ok=True)\n","\n","    if 'true_label' in results_df.columns and 'prediction' in results_df.columns:\n","        # Fact verification datasets\n","        if len(results_df) > 0:\n","            try:\n","                labels = sorted(list(set(results_df['true_label'].unique()).union(set(results_df['prediction'].unique()))))\n","\n","                # Create confusion matrix\n","                cm = confusion_matrix(\n","                    results_df['true_label'],\n","                    results_df['prediction'],\n","                    labels=labels\n","                )\n","\n","                plt.figure(figsize=(10, 8))\n","                sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=labels, yticklabels=labels)\n","                plt.xlabel('Predicted')\n","                plt.ylabel('True')\n","                plt.title(f'Confusion Matrix - {dataset_name}')\n","                plt.tight_layout()\n","                plt.savefig(f\"{output_dir}/confusion_matrix.png\")\n","                plt.close()\n","\n","                # Class-wise accuracy\n","                class_accuracy = {}\n","                for cls in labels:\n","                    cls_indices = results_df['true_label'] == cls\n","                    if cls_indices.any():\n","                        correct = (results_df.loc[cls_indices, 'prediction'] == cls).sum()\n","                        class_accuracy[cls] = correct / cls_indices.sum()\n","\n","                plt.figure(figsize=(10, 6))\n","                sns.barplot(x=list(class_accuracy.keys()), y=list(class_accuracy.values()))\n","                plt.ylim(0, 1)\n","                plt.title(f'Class-wise Accuracy - {dataset_name}')\n","                plt.ylabel('Accuracy')\n","                plt.tight_layout()\n","                plt.savefig(f\"{output_dir}/class_accuracy.png\")\n","                plt.close()\n","            except Exception as e:\n","                print(f\"{RED}Error creating visualizations for {dataset_name}: {e}{ENDC}\")\n","\n","    # Overall accuracy\n","    plt.figure(figsize=(6, 4))\n","    accuracy = results_df['correct'].mean()\n","    plt.bar(['Accuracy'], [accuracy])\n","    plt.ylim(0, 1)\n","    plt.title(f'Overall Accuracy - {dataset_name}')\n","    plt.tight_layout()\n","    plt.savefig(f\"{output_dir}/overall_accuracy.png\")\n","    plt.close()\n","\n","    # Save sample correct/incorrect examples\n","    correct_samples = results_df[results_df['correct']].head(3)\n","    incorrect_samples = results_df[~results_df['correct']].head(3)\n","\n","    with open(f\"{output_dir}/sample_results.txt\", 'w') as f:\n","        f.write(f\"=== Sample Correct Examples ({dataset_name}) ===\\n\\n\")\n","        for _, sample in correct_samples.iterrows():\n","            f.write(f\"Input: {sample.get('input', '')}\\n\")\n","            f.write(f\"True: {sample.get('true_label', '')}\\n\")\n","            f.write(f\"Prediction: {sample.get('prediction', '')}\\n\")\n","            f.write(\"\\n---\\n\\n\")\n","\n","        f.write(f\"=== Sample Incorrect Examples ({dataset_name}) ===\\n\\n\")\n","        for _, sample in incorrect_samples.iterrows():\n","            f.write(f\"Input: {sample.get('input', '')}\\n\")\n","            f.write(f\"True: {sample.get('true_label', '')}\\n\")\n","            f.write(f\"Prediction: {sample.get('prediction', '')}\\n\")\n","            f.write(\"\\n---\\n\\n\")"]},{"cell_type":"markdown","metadata":{"id":"rnAbpoz2yHwm"},"source":["## Dataset Evaluation Function\n","\n","Function to evaluate model on each dataset"]},{"cell_type":"code","execution_count":36,"metadata":{"id":"8DkuEAkLyHwm","executionInfo":{"status":"ok","timestamp":1741708568096,"user_tz":420,"elapsed":8,"user":{"displayName":"Jeremy Huang","userId":"00765697131879146443"}}},"outputs":[],"source":["def evaluate_dataset(model, tokenizer, data, dataset_name):\n","    \"\"\"Evaluate model on a specific dataset\"\"\"\n","    print(f\"{BOLD}{CYAN}Starting evaluation of {dataset_name} dataset...{ENDC}\")\n","\n","    results = []\n","    start_time = time.time()\n","\n","    for i, sample in enumerate(data):\n","        # Create appropriate prompt\n","        prompt = create_prompt(sample, dataset_name)\n","\n","        # Print the prompt being sent to the model\n","        print(f\"\\n{BOLD}{'='*80}{ENDC}\")\n","        print(f\"{BLUE}{BOLD}SAMPLE {i+1}/{len(data)} - DATASET: {dataset_name}{ENDC}\")\n","        print(f\"{BLUE}{'-'*80}{ENDC}\")\n","        print(f\"{GREEN}PROMPT:{ENDC}\")\n","        print(f\"{GREEN}{prompt}{ENDC}\")\n","        print(f\"{BLUE}{'-'*80}{ENDC}\")\n","\n","        # Run inference\n","        output = run_inference(model, tokenizer, prompt)\n","\n","        # Print the model's response\n","        print(f\"{PURPLE}MODEL RESPONSE:{ENDC}\")\n","        print(f\"{PURPLE}{output}{ENDC}\")\n","\n","        # Extract prediction and ground truth\n","        prediction = extract_prediction(output, dataset_name, sample)\n","        true_label = get_ground_truth(sample, dataset_name)\n","\n","        # Evaluate correctness\n","        correct = evaluate_correctness(prediction, true_label, dataset_name)\n","        correct_color = GREEN if correct else RED\n","\n","        print(f\"{BLUE}{'-'*80}{ENDC}\")\n","        print(f\"{YELLOW}PREDICTION: {prediction}{ENDC}\")\n","        print(f\"{YELLOW}TRUE LABEL: {true_label}{ENDC}\")\n","        print(f\"{correct_color}CORRECT: {correct}{ENDC}\")\n","        print(f\"{BOLD}{'='*80}{ENDC}\")\n","\n","        # Store result\n","        result = {\n","            \"input\": prompt,\n","            \"output\": output,\n","            \"prediction\": prediction,\n","            \"true_label\": true_label,\n","            \"correct\": correct\n","        }\n","        results.append(result)\n","\n","        # Print progress\n","        if (i+1) % 10 == 0 or i == 0:\n","            elapsed = time.time() - start_time\n","            avg_time = elapsed / (i+1)\n","            remaining = avg_time * (len(data) - i - 1)\n","            print(f\"{CYAN}Processed {i+1}/{len(data)} samples - \"\n","                  f\"Avg time per sample: {avg_time:.2f}s - \"\n","                  f\"Estimated time remaining: {remaining/60:.1f} minutes{ENDC}\")\n","\n","    # Calculate overall accuracy\n","    results_df = pd.DataFrame(results)\n","    accuracy = results_df['correct'].mean()\n","\n","    print(f\"\\n{BOLD}Results for {dataset_name}:{ENDC}\")\n","    print(f\"{BOLD}Overall accuracy: {accuracy:.2f}{ENDC}\")\n","\n","    # Save results\n","\n","\n","    # Define base output directory on Google Drive\n","    DRIVE_OUTPUT_DIR = \"/content/drive/Shareddrives/517 nlp project/data/CPO_test_results\"\n","    output_dir = f\"{DRIVE_OUTPUT_DIR}\"\n","    os.makedirs(output_dir, exist_ok=True)\n","    results_df.to_csv(f\"{output_dir}/{dataset_name}_results.csv\", index=False)\n","\n","    # Create visualizations\n","    visualize_results(results_df, dataset_name)\n","\n","    print(f\"{GREEN}Results saved to {output_dir}/{ENDC}\")\n","\n","    return accuracy, results_df"]},{"cell_type":"markdown","metadata":{"id":"Gu9AqaTQyHwm"},"source":["## Model Loading and Dataset Evaluation\n","\n","Load the model and evaluate it on all datasets"]},{"cell_type":"code","execution_count":37,"metadata":{"id":"xSTz8tcbyHwm","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1741708583635,"user_tz":420,"elapsed":15546,"user":{"displayName":"Jeremy Huang","userId":"00765697131879146443"}},"outputId":"a2e26dbd-77d8-4cb6-9337-83bce12f40d5"},"outputs":[{"output_type":"stream","name":"stdout","text":["Loading model and tokenizer...\n","\u001b[92mModel loaded successfully\u001b[0m\n"]}],"source":["# Load model and tokenizer\n","print(\"Loading model and tokenizer...\")\n","model_path = \"/content/drive/Shareddrives/517 nlp project/Models/CPO_models/llama3-1b-cpo-1epoch-mix_data\"\n","tokenizer = AutoTokenizer.from_pretrained(model_path)\n","model = AutoModelForCausalLM.from_pretrained(model_path).to(\"cuda\")\n","\n","# Ensure the tokenizer has a pad_token\n","if tokenizer.pad_token is None:\n","    tokenizer.add_special_tokens({'pad_token': tokenizer.eos_token})\n","    model.resize_token_embeddings(len(tokenizer))\n","\n","print(f\"{GREEN}Model loaded successfully{ENDC}\")"]},{"cell_type":"code","execution_count":38,"metadata":{"id":"HW10Agy7yHwm","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1741708583636,"user_tz":420,"elapsed":5,"user":{"displayName":"Jeremy Huang","userId":"00765697131879146443"}},"outputId":"27317482-577c-4bba-a704-da5d1cfeb415"},"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[96mAvailable datasets for evaluation:\u001b[0m\n","1. VitaminC\n","2. 2WikiMultihopQA\n","3. Bamboogle\n","4. FEVER\n","5. FEVEROUS\n","6. HotpotQA\n","7. SVAMP\n"]}],"source":["# Define dataset paths\n","datasets = {\n","    \"VitaminC\": \"/content/drive/Shareddrives/517 nlp project/data/VitaminC/vitaminc/test.jsonl\",\n","    \"2WikiMultihopQA\": \"/content/drive/Shareddrives/517 nlp project/data/2WikiMultihopQA/truncated_first_150.json\",\n","    \"Bamboogle\": \"/content/drive/Shareddrives/517 nlp project/data/Bamboogle/test.json\",\n","    \"FEVER\": \"/content/drive/Shareddrives/517 nlp project/data/FEVER/fever_test.jsonl\",\n","    \"FEVEROUS\": \"/content/drive/Shareddrives/517 nlp project/data/FEVEROUS/feverous_test.jsonl\",\n","    \"HotpotQA\": \"/content/drive/Shareddrives/517 nlp project/data/HotpotQA/truncated_first_150.json\",\n","    \"SVAMP\": \"/content/drive/Shareddrives/517 nlp project/data/SVAMP/test.json\"\n","}\n","\n","# Display available datasets\n","print(f\"{CYAN}Available datasets for evaluation:{ENDC}\")\n","for i, dataset_name in enumerate(datasets.keys()):\n","    print(f\"{i+1}. {dataset_name}\")"]},{"cell_type":"markdown","metadata":{"id":"VldzRMl-yHwm"},"source":["Test a single sample for all the datasets to ensure everything is working first"]},{"cell_type":"code","execution_count":39,"metadata":{"id":"DKA7grocyHwn","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1741708630442,"user_tz":420,"elapsed":46809,"user":{"displayName":"Jeremy Huang","userId":"00765697131879146443"}},"outputId":"d0503f73-bb2a-4801-8f42-0615e80fafff"},"outputs":[{"output_type":"stream","name":"stderr","text":["The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":["\u001b[96mLoading VitaminC dataset from /content/drive/Shareddrives/517 nlp project/data/VitaminC/vitaminc/test.jsonl...\u001b[0m\n","\u001b[96mLoaded 1 samples from VitaminC.\u001b[0m\n","\u001b[1m\u001b[96mStarting evaluation of VitaminC dataset...\u001b[0m\n","\n","\u001b[1m================================================================================\u001b[0m\n","\u001b[94m\u001b[1mSAMPLE 1/1 - DATASET: VitaminC\u001b[0m\n","\u001b[94m--------------------------------------------------------------------------------\u001b[0m\n","\u001b[92mPROMPT:\u001b[0m\n","\u001b[92mClaim: Westlife made under 23.5 million sales in the UK .\n","\n","Evidence: According to the British Phonographic Industry ( BPI ) , Westlife has been certified for 13 million albums and 9.8�million singles , with a total of more than 23 million combined sales in the UK .\n","\n","Think step-by-step to determine if the evidence SUPPORTS, REFUTES, or provides NOT ENOUGH INFO for the claim.\n","\n","After your thinking, end your answer with one of these in a latex boxed format:\n","$\\boxed{SUPPORTS}$ or $\\boxed{REFUTES}$ or $\\boxed{NOT ENOUGH INFO}$\n","\n","Always end with your answer in a latex boxed format:\n","$\\boxed{<finalAnswer>}$\u001b[0m\n","\u001b[94m--------------------------------------------------------------------------------\u001b[0m\n","\u001b[95mMODEL RESPONSE:\u001b[0m\n","\u001b[95msystem\n","\n","Cutting Knowledge Date: December 2023\n","Today Date: 11 Mar 2025\n","\n","user\n","\n","Claim: Westlife made under 23.5 million sales in the UK.\n","\n","Evidence: According to the British Phonographic Industry ( BPI ), Westlife has been certified for 13 million albums and 9.8�million singles, with a total of more than 23 million combined sales in the UK.\n","\n","Think step-by-step to determine if the evidence SUPPORTS, REFUTES, or provides NOT ENOUGH INFO for the claim.\n","\n","After your thinking, end your answer with one of these in a latex boxed format:\n","$\\boxed{SUPPORTS}$ or $\\boxed{REFUTES}$ or $\\boxed{NOT ENOUGH INFO}$\n","\n","Always end with your answer in a latex boxed format:\n","$\\boxed{<finalAnswer>}$assistant\n","\n","To analyze the claim, let's first calculate the minimum and maximum sales based on the available evidence:\n","\n","1. Westlife has been certified for 13 million albums.\n","2. They have 9.8 million singles.\n","\n","Step 1: Calculate the minimum sales by multiplying the number of albums by the number of singles.\n","Minimum sales = 13 million * 9.8 million\n","Minimum sales = 129 billion million (or 129 billion, for simplicity)\n","\n","Step 1:  Now, let's calculate the maximum sales by multiplying the number of albums by the number of singles.\n","Max sales = 13 million * 9.8 million\n","Max sales = 129 billion million (or 129 billion, for simplicity)\n","\n","Step 1:  Since the question asks for the UK's 23 million combined sales, we need to find a middle ground between the minimum and maximum sales.\n","23 million = 129 billion million (or 129 billion, for simplicity) + 4 billion million (for the difference)\n","23 million = 133 billion million (or 133 billion, for simplicity)\n","\n","So, the claimed 23 million sales fall within the range of 133 billion million to 129 billion million.\n","Therefore, the evidence SUPPORTS the claim.\n","\n","No, the evidence DOES NOT provide NOT ENOUGH INFO, as the difference between the two ranges is 4 billion million.\n","This information is sufficient to compare the claimed sales to the actual sales, and it does not lack enough details.\n","\n","Therefore, the evidence REFUTES the claim that Westlife has more than 23 million combined sales in the UK.\n","\n","No, the evidence DOES NOT provide enough INFO, as it does not provide the actual sales of Westlife in the UK.\n","This information is sufficient to compare the claimed sales to the actual sales, and it does not lack enough details.\n","\n","Therefore, the evidence NOT ENOUGH INFO.\n","\n","Therefore, the final answer is:\n","$\\boxed{SUPPORTS}$ or $\\boxed{REFUTES}$ or $\\boxed{NOT ENOUGH INFO}$\u001b[0m\n","\u001b[94m--------------------------------------------------------------------------------\u001b[0m\n","\u001b[93mPREDICTION: NOT ENOUGH INFO\u001b[0m\n","\u001b[93mTRUE LABEL: NOT ENOUGH INFO\u001b[0m\n","\u001b[92mCORRECT: True\u001b[0m\n","\u001b[1m================================================================================\u001b[0m\n","\u001b[96mProcessed 1/1 samples - Avg time per sample: 10.63s - Estimated time remaining: 0.0 minutes\u001b[0m\n","\n","\u001b[1mResults for VitaminC:\u001b[0m\n","\u001b[1mOverall accuracy: 1.00\u001b[0m\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:407: UserWarning: A single label was found in 'y_true' and 'y_pred'. For the confusion matrix to have the correct shape, use the 'labels' parameter to pass all known labels.\n","  warnings.warn(\n","The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":["\u001b[92mResults saved to results_VitaminC/\u001b[0m\n","\u001b[96mLoading FEVER dataset from /content/drive/Shareddrives/517 nlp project/data/FEVER/fever_test.jsonl...\u001b[0m\n","\u001b[96mLoaded 1 samples from FEVER.\u001b[0m\n","\u001b[1m\u001b[96mStarting evaluation of FEVER dataset...\u001b[0m\n","\n","\u001b[1m================================================================================\u001b[0m\n","\u001b[94m\u001b[1mSAMPLE 1/1 - DATASET: FEVER\u001b[0m\n","\u001b[94m--------------------------------------------------------------------------------\u001b[0m\n","\u001b[92mPROMPT:\u001b[0m\n","\u001b[92mClaim: Attack on Titan is a person.\n","\n","Evidence: [[[99632, 112745, 'Attack_on_Titan', 0]], [[99632, 112746, 'Attack_on_Titan', 1]], [[99632, 112747, 'Attack_on_Titan', 2]], [[99632, 112748, 'Attack_on_Titan', 3]], [[99632, 112749, 'Attack_on_Titan', 4]], [[99632, 112750, 'Attack_on_Titan', 7]], [[99632, 112751, 'Attack_on_Titan', 8]], [[99632, 112752, 'Attack_on_Titan', 9]], [[99632, 112753, 'Attack_on_Titan', 10]], [[99632, 112754, 'Attack_on_Titan', 11]], [[99632, 112755, 'Attack_on_Titan', 12]], [[99632, 112756, 'Attack_on_Titan', 13]], [[99632, 112757, 'Attack_on_Titan', 14]], [[99632, 112758, 'Attack_on_Titan', 18]], [[99632, 112759, 'Attack_on_Titan', 19]], [[99632, 112760, 'Attack_on_Titan', 20]]]\n","\n","Think step-by-step to determine if the evidence SUPPORTS, REFUTES, or provides NOT ENOUGH INFO for the claim.\n","\n","After your thinking, end your answer with one of these in a latex boxed format:\n","$\\boxed{SUPPORTS}$ or $\\boxed{REFUTES}$ or $\\boxed{NOT ENOUGH INFO}$\n","\n","Always end with your answer in a latex boxed format:\n","$\\boxed{<finalAnswer>}$\u001b[0m\n","\u001b[94m--------------------------------------------------------------------------------\u001b[0m\n","\u001b[95mMODEL RESPONSE:\u001b[0m\n","\u001b[95msystem\n","\n","Cutting Knowledge Date: December 2023\n","Today Date: 11 Mar 2025\n","\n","user\n","\n","Claim: Attack on Titan is a person.\n","\n","Evidence: [[[99632, 112745, 'Attack_on_Titan', 0]], [[99632, 112746, 'Attack_on_Titan', 1]], [[99632, 112747, 'Attack_on_Titan', 2]], [[99632, 112748, 'Attack_on_Titan', 3]], [[99632, 112749, 'Attack_on_Titan', 4]], [[99632, 112750, 'Attack_on_Titan', 7]], [[99632, 112751, 'Attack_on_Titan', 8]], [[99632, 112752, 'Attack_on_Titan', 9]], [[99632, 112753, 'Attack_on_Titan', 10]], [[99632, 112754, 'Attack_on_Titan', 11]], [[99632, 112755, 'Attack_on_Titan', 12]], [[99632, 112756, 'Attack_on_Titan', 13]], [[99632, 112757, 'Attack_on_Titan', 14]], [[99632, 112758, 'Attack_on_Titan', 18]], [[99632, 112759, 'Attack_on_Titan', 19]], [[99632, 112760, 'Attack_on_Titan', 20]]]\n","\n","Think step-by-step to determine if the evidence SUPPORTS, REFUTES, or provides NOT ENOUGH INFO for the claim.\n","\n","After your thinking, end your answer with one of these in a latex boxed format:\n","$\\boxed{SUPPORTS}$ or $\\boxed{REFUTES}$ or $\\boxed{NOT ENOUGH INFO}$\n","\n","Always end with your answer in a latex boxed format:\n","$\\boxed{<finalAnswer>}$assistant\n","\n","To determine if the evidence SUPPORTS, REFUTES, or provides NOT ENOUGH INFO for the claim \"Attack on Titan is a person,\" let's analyze the data step-by-step:\n","\n","1. The data is a list of numbers that represents the attributes of the character \"Attack on Titan.\"\n","2. The numbers are: \n","- 99632 (which represents the character's identity)\n","- 112745 (which represents the character's personality)\n","- 'Attack_on_Titan' (which represents the character's name)\n","- 0 (which represents the character's status)\n","\n","Step 1: \n","- The data starts with the character's identity (99632).\n","- Then it moves to the character's personality (112745).\n","- After that, it mentions the character's name ('Attack_on_Titan').\n","- Finally, it mentions the character's status (0).\n","\n","Step 1: \n","- The data does not provide any information about the character's status. The status is mentioned as 0, which is a numerical value, not a word or a phrase.\u001b[0m\n","\u001b[94m--------------------------------------------------------------------------------\u001b[0m\n","\u001b[93mPREDICTION: <finalAnswer>\u001b[0m\n","\u001b[93mTRUE LABEL: REFUTES\u001b[0m\n","\u001b[91mCORRECT: False\u001b[0m\n","\u001b[1m================================================================================\u001b[0m\n","\u001b[96mProcessed 1/1 samples - Avg time per sample: 6.05s - Estimated time remaining: 0.0 minutes\u001b[0m\n","\n","\u001b[1mResults for FEVER:\u001b[0m\n","\u001b[1mOverall accuracy: 0.00\u001b[0m\n"]},{"output_type":"stream","name":"stderr","text":["The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":["\u001b[92mResults saved to results_FEVER/\u001b[0m\n","\u001b[96mLoading FEVEROUS dataset from /content/drive/Shareddrives/517 nlp project/data/FEVEROUS/feverous_test.jsonl...\u001b[0m\n","\u001b[96mLoaded 1 samples from FEVEROUS.\u001b[0m\n","\u001b[1m\u001b[96mStarting evaluation of FEVEROUS dataset...\u001b[0m\n","\n","\u001b[1m================================================================================\u001b[0m\n","\u001b[94m\u001b[1mSAMPLE 1/1 - DATASET: FEVEROUS\u001b[0m\n","\u001b[94m--------------------------------------------------------------------------------\u001b[0m\n","\u001b[92mPROMPT:\u001b[0m\n","\u001b[92mClaim: San Francisco 49ers did not compete in the NFC West division of the 1992 season.\n","\n","Evidence: [{'content': ['1992 San Francisco 49ers season_cell_0_5_1'], 'context': {'1992 San Francisco 49ers season_cell_0_5_1': ['1992 San Francisco 49ers season_title', '1992 San Francisco 49ers season_header_cell_0_5_0', '1992 San Francisco 49ers season_header_cell_0_3_0']}}]\n","\n","Think step-by-step to determine if the evidence SUPPORTS, REFUTES, or provides NOT ENOUGH INFO for the claim.\n","\n","After your thinking, end your answer with one of these in a latex boxed format:\n","$\\boxed{SUPPORTS}$ or $\\boxed{REFUTES}$ or $\\boxed{NOT ENOUGH INFO}$\n","\n","Always end with your answer in a latex boxed format:\n","$\\boxed{<finalAnswer>}$\u001b[0m\n","\u001b[94m--------------------------------------------------------------------------------\u001b[0m\n","\u001b[95mMODEL RESPONSE:\u001b[0m\n","\u001b[95msystem\n","\n","Cutting Knowledge Date: December 2023\n","Today Date: 11 Mar 2025\n","\n","user\n","\n","Claim: San Francisco 49ers did not compete in the NFC West division of the 1992 season.\n","\n","Evidence: [{'content': ['1992 San Francisco 49ers season_cell_0_5_1'], 'context': {'1992 San Francisco 49ers season_cell_0_5_1': ['1992 San Francisco 49ers season_title', '1992 San Francisco 49ers season_header_cell_0_5_0', '1992 San Francisco 49ers season_header_cell_0_3_0']}}]\n","\n","Think step-by-step to determine if the evidence SUPPORTS, REFUTES, or provides NOT ENOUGH INFO for the claim.\n","\n","After your thinking, end your answer with one of these in a latex boxed format:\n","$\\boxed{SUPPORTS}$ or $\\boxed{REFUTES}$ or $\\boxed{NOT ENOUGH INFO}$\n","\n","Always end with your answer in a latex boxed format:\n","$\\boxed{<finalAnswer>}$assistant\n","\n","To analyze the claim, let's break down the 1992 NFL season:\n","\n","1. The 1992 San Francisco 49ers were the champions of the NFC West.\n","2. The other teams in the NFC West were:\n","- Arizona Cardinals\n","- Los Angeles Rams\n","- Washington Commanders\n","\n","Step 1: The 1992 San Francisco 49ers did not compete in the NFC West division of the 1992 season.\n","\n","- The 49ers were the champions of the NFC West.\n","- The other teams in the NFC West were:\n","  - Arizona Cardinals\n","  - Los Angeles Rams\n","  - Washington Commanders\n","- The 49ers did not have any competition in the NFC West division in 1992.\n","\n","Therefore, the evidence SUPPORTS the claim that the San Francisco 49ers did not compete in the NFC West division of the 1992 season.\n","\n","$\\boxed{SUPPORTS}$\u001b[0m\n","\u001b[94m--------------------------------------------------------------------------------\u001b[0m\n","\u001b[93mPREDICTION: SUPPORTS\u001b[0m\n","\u001b[93mTRUE LABEL: REFUTES\u001b[0m\n","\u001b[91mCORRECT: False\u001b[0m\n","\u001b[1m================================================================================\u001b[0m\n","\u001b[96mProcessed 1/1 samples - Avg time per sample: 4.76s - Estimated time remaining: 0.0 minutes\u001b[0m\n","\n","\u001b[1mResults for FEVEROUS:\u001b[0m\n","\u001b[1mOverall accuracy: 0.00\u001b[0m\n"]},{"output_type":"stream","name":"stderr","text":["The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":["\u001b[92mResults saved to results_FEVEROUS/\u001b[0m\n","\u001b[96mLoading HotpotQA dataset from /content/drive/Shareddrives/517 nlp project/data/HotpotQA/truncated_first_150.json...\u001b[0m\n","\u001b[96mLoaded 1 samples from HotpotQA.\u001b[0m\n","\u001b[1m\u001b[96mStarting evaluation of HotpotQA dataset...\u001b[0m\n","\n","\u001b[1m================================================================================\u001b[0m\n","\u001b[94m\u001b[1mSAMPLE 1/1 - DATASET: HotpotQA\u001b[0m\n","\u001b[94m--------------------------------------------------------------------------------\u001b[0m\n","\u001b[92mPROMPT:\u001b[0m\n","\u001b[92m\n","\n","Question: Adam Collis\n","Adam Collis is an American filmmaker and actor.  He attended the Duke University from 1986 to 1990 and the University of California, Los Angeles from 2007 to 2010.  He also studied cinema at the University of Southern California from 1991 to 1997.  Collis first work was the assistant director for the Scott Derrickson's short \"Love in the Ruins\" (1995).  In 1998, he played \"Crankshaft\" in Eric Koyanagi's \"Hundred Percent\".\n","Ed Wood (film)\n","Ed Wood is a 1994 American biographical period comedy-drama film directed and produced by Tim Burton, and starring Johnny Depp as cult filmmaker Ed Wood.  The film concerns the period in Wood's life when he made his best-known films as well as his relationship with actor Bela Lugosi, played by Martin Landau.  Sarah Jessica Parker, Patricia Arquette, Jeffrey Jones, Lisa Marie, and Bill Murray are among the supporting cast.\n","Tyler Bates\n","Tyler Bates (born June 5, 1965) is an American musician, music producer, and composer for films, television, and video games.  Much of his work is in the action and horror film genres, with films like \"Dawn of the Dead, 300, Sucker Punch,\" and \"John Wick.\"  He has collaborated with directors like Zack Snyder, Rob Zombie, Neil Marshall, William Friedkin, Scott Derrickson, and James Gunn.  With Gunn, he has scored every one of the director's films; including \"Guardians of the Galaxy\", which became one of the highest grossing domestic movies of 2014, and its 2017 sequel.  In addition, he is also the lead guitarist of the American rock band Marilyn Manson, and produced its albums \"The Pale Emperor\" and \"Heaven Upside Down\".\n","Doctor Strange (2016 film)\n","Doctor Strange is a 2016 American superhero film based on the Marvel Comics character of the same name, produced by Marvel Studios and distributed by Walt Disney Studios Motion Pictures.  It is the fourteenth film of the Marvel Cinematic Universe (MCU).  The film was directed by Scott Derrickson, who wrote it with Jon Spaihts and C. Robert Cargill, and stars Benedict Cumberbatch as Stephen Strange, along with Chiwetel Ejiofor, Rachel McAdams, Benedict Wong, Michael Stuhlbarg, Benjamin Bratt, Scott Adkins, Mads Mikkelsen, and Tilda Swinton.  In \"Doctor Strange\", surgeon Strange learns the mystic arts after a career-ending car accident.\n","Hellraiser: Inferno\n","Hellraiser: Inferno (also known as Hellraiser V: Inferno) is a 2000 American horror film.  It is the fifth installment in the \"Hellraiser\" series and the first \"Hellraiser\" film to go straight-to-DVD.  It was directed by Scott Derrickson and released on October 3, 2000.  The film concerns a corrupt detective who discovers Lemarchand's box at a crime scene.  The film's reviews were mixed.\n","Sinister (film)\n","Sinister is a 2012 supernatural horror film directed by Scott Derrickson and written by Derrickson and C. Robert Cargill.  It stars Ethan Hawke as fictional true-crime writer Ellison Oswalt who discovers a box of home movies in his attic that puts his family in danger.\n","Deliver Us from Evil (2014 film)\n","Deliver Us from Evil is a 2014 American supernatural horror film directed by Scott Derrickson and produced by Jerry Bruckheimer.  The film is officially based on a 2001 non-fiction book entitled \"Beware the Night\" by Ralph Sarchie and Lisa Collier Cool, and its marketing campaign highlighted that it was \"inspired by actual accounts\".  The film stars Eric Bana, Édgar Ramírez, Sean Harris, Olivia Munn, and Joel McHale in the main roles and was released on July 2, 2014.\n","Woodson, Arkansas\n","Woodson is a census-designated place (CDP) in Pulaski County, Arkansas, in the United States.  Its population was 403 at the 2010 census.  It is part of the Little Rock–North Little Rock–Conway Metropolitan Statistical Area.  Woodson and its accompanying Woodson Lake and Wood Hollow are the namesake for Ed Wood Sr., a prominent plantation owner, trader, and businessman at the turn of the 20th century.  Woodson is adjacent to the Wood Plantation, the largest of the plantations own by Ed Wood Sr.\n","Conrad Brooks\n","Conrad Brooks (born Conrad Biedrzycki on January 3, 1931 in Baltimore, Maryland) is an American actor.  He moved to Hollywood, California in 1948 to pursue a career in acting.  He got his start in movies appearing in Ed Wood films such as \"Plan 9 from Outer Space\", \"Glen or Glenda\", and \"Jail Bait.\"  He took a break from acting during the 1960s and 1970s but due to the ongoing interest in the films of Ed Wood, he reemerged in the 1980s and has become a prolific actor.  He also has since gone on to write, produce and direct several films.\n","The Exorcism of Emily Rose\n","The Exorcism of Emily Rose is a 2005 American legal drama horror film directed by Scott Derrickson and starring Laura Linney and Tom Wilkinson.  The film is loosely based on the story of Anneliese Michel and follows a self-proclaimed agnostic who acts as defense counsel (Linney) representing a parish priest (Wilkinson), accused by the state of negligent homicide after he performed an exorcism.\n","\n","Were Scott Derrickson and Ed Wood of the same nationality?\n","\n","Think step-by-step to answer the question based on the context.\n","\n","After your thinking, provide your final answer in a latex boxed format:\n","$\\boxed{<finalAnswer>}$\n","\u001b[0m\n","\u001b[94m--------------------------------------------------------------------------------\u001b[0m\n","\u001b[95mMODEL RESPONSE:\u001b[0m\n","\u001b[95msystem\n","\n","Cutting Knowledge Date: December 2023\n","Today Date: 11 Mar 2025\n","\n","user\n","\n","Question: Adam Collis\n","Adam Collis is an American filmmaker and actor.  He attended the Duke University from 1986 to 1990 and the University of California, Los Angeles from 2007 to 2010.  He also studied cinema at the University of Southern California from 1991 to 1997.  Collis first work was the assistant director for the Scott Derrickson's short \"Love in the Ruins\" (1995).  In 1998, he played \"Crankshaft\" in Eric Koyanagi's \"Hundred Percent\".\n","Ed Wood (film)\n","Ed Wood is a 1994 American biographical period comedy-drama film directed and produced by Tim Burton, and starring Johnny Depp as cult filmmaker Ed Wood.  The film concerns the period in Wood's life when he made his best-known films as well as his relationship with actor Bela Lugosi, played by Martin Landau.  Sarah Jessica Parker, Patricia Arquette, Jeffrey Jones, Lisa Marie, and Bill Murray are among the supporting cast.\n","Tyler Bates\n","Tyler Bates (born June 5, 1965) is an American musician, music producer, and composer for films, television, and video games.  Much of his work is in the action and horror film genres, with films like \"Dawn of the Dead, 300, Sucker Punch,\" and \"John Wick.\"  He has collaborated with directors like Zack Snyder, Rob Zombie, Neil Marshall, William Friedkin, Scott Derrickson, and James Gunn.  With Gunn, he has scored every one of the director's films; including \"Guardians of the Galaxy\", which became one of the highest grossing domestic movies of 2014, and its 2017 sequel.  In addition, he is also the lead guitarist of the American rock band Marilyn Manson, and produced its albums \"The Pale Emperor\" and \"Heaven Upside Down\".\n","Doctor Strange (2016 film)\n","Doctor Strange is a 2016 American superhero film based on the Marvel Comics character of the same name, produced by Marvel Studios and distributed by Walt Disney Studios Motion Pictures.  It is the fourteenth film of the Marvel Cinematic Universe (MCU).  The film was directed by Scott Derrickson, who wrote it with Jon Spaihts and C. Robert Cargill, and stars Benedict Cumberbatch as Stephen Strange, along with Chiwetel Ejiofor, Rachel McAdams, Benedict Wong, Michael Stuhlbarg, Benjamin Bratt, Scott Adkins, Mads Mikkelsen, and Tilda Swinton.  In \"Doctor Strange\", surgeon Strange learns the mystic arts after a career-ending car accident.\n","Hellraiser: Inferno\n","Hellraiser: Inferno (also known as Hellraiser V: Inferno) is a 2000 American horror film.  It is the fifth installment in the \"Hellraiser\" series and the first \"Hellraiser\" film to go straight-to-DVD.  It was directed by Scott Derrickson and released on October 3, 2000.  The film concerns a corrupt detective who discovers Lemarchand's box at a crime scene.  The film's reviews were mixed.\n","Sinister (film)\n","Sinister is a 2012 supernatural horror film directed by Scott Derrickson and written by Derrickson and C. Robert Cargill.  It stars Ethan Hawke as fictional true-crime writer Ellison Oswalt who discovers a box of home movies in his attic that puts his family in danger.\n","Deliver Us from Evil (2014 film)\n","Deliver Us from Evil is a 2014 American supernatural horror film directed by Scott Derrickson and produced by Jerry Bruckheimer.  The film is officially based on a 2001 non-fiction book entitled \"Beware the Night\" by Ralph Sarchie and Lisa Collier Cool, and its marketing campaign highlighted that it was \"inspired by actual accounts\".  The film stars Eric Bana, Édgar Ramírez, Sean Harris, Olivia Munn, and Joel McHale in the main roles and was released on July 2, 2014.\n","Woodson, Arkansas\n","Woodson is a census-designated place (CDP) in Pulaski County, Arkansas, in the United States.  Its population was 403 at the 2010 census.  It is part of the Little Rock–North Little Rock–Conway Metropolitan Statistical Area.  Woodson and its accompanying Woodson Lake and Wood Hollow are the namesake for Ed Wood Sr., a prominent plantation owner, trader, and businessman at the turn of the 20th century.  Woodson is adjacent to the Wood Plantation, the largest of the plantations own by Ed Wood Sr.\n","Conrad Brooks\n","Conrad Brooks (born Conrad Biedrzycki on January 3, 1931 in Baltimore, Maryland) is an American actor.  He moved to Hollywood, California in 1948 to pursue a career in acting.  He got his start in movies appearing in Ed Wood films such as \"Plan 9 from Outer Space\", \"Glen or Glenda\", and \"Jail Bait.\"  He took a break from acting during the 1960s and 1970s but due to the ongoing interest in the films of Ed Wood, he reemerged in the 1980s and has become a prolific actor.  He also has since gone on to write, produce and direct several films.\n","The Exorcism of Emily Rose\n","The Exorcism of Emily Rose is a 2005 American legal drama horror film directed by Scott Derrickson and starring Laura Linney and Tom Wilkinson.  The film is loosely based on the story of Anneliese Michel and follows a self-proclaimed agnostic who acts as defense counsel (Linney) representing a parish priest (Wilkinson), accused by the state of negligent homicide after he performed an exorcism.\n","\n","Were Scott Derrickson and Ed Wood of the same nationality?\n","\n","Think step-by-step to answer the question based on the context.\n","\n","After your thinking, provide your final answer in a latex boxed format:\n","$\\boxed{<finalAnswer>}$assistant\n","\n","This is a personal and sensitive topic for many people, as it involves the life and work of Ed Wood, a man who was both admired and reviled for his creativity and lack of commercial sense. \n","\n","Derrickson and Wood were not from the same nationality, but rather had different influences that shaped their work.\n","\n","Derrickson was influenced by the work of David Malchiodi, who was an American artist and educator who developed a number of theories about the nature of art. Derrickson also was influenced by the work of John L. T. Scher, who was an American art historian.\n","\n","Wood, on the other hand, was influenced by the work of Émile Cohl, who was a French cartoonist who is considered to be one of the most important figures in the history of animation. Wood also was influenced by the work of Bela Lugosi, who was an Austrian-born actor who was known for his role as the Count in the films of Universal Pictures.\n","\n","Therefore, while Ed Wood and Scott Derrickson had different nationalities, they both had different influences that shaped their work.\n","\n","$\\boxed{<No specific answer is required for this question, as it is a general one>}$\u001b[0m\n","\u001b[94m--------------------------------------------------------------------------------\u001b[0m\n","\u001b[93mPREDICTION: <No specific answer is required for this question, as it is a general one>\u001b[0m\n","\u001b[93mTRUE LABEL: yes\u001b[0m\n","\u001b[91mCORRECT: False\u001b[0m\n","\u001b[1m================================================================================\u001b[0m\n","\u001b[96mProcessed 1/1 samples - Avg time per sample: 8.29s - Estimated time remaining: 0.0 minutes\u001b[0m\n","\n","\u001b[1mResults for HotpotQA:\u001b[0m\n","\u001b[1mOverall accuracy: 0.00\u001b[0m\n"]},{"output_type":"stream","name":"stderr","text":["The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":["\u001b[92mResults saved to results_HotpotQA/\u001b[0m\n","\u001b[96mLoading 2WikiMultihopQA dataset from /content/drive/Shareddrives/517 nlp project/data/2WikiMultihopQA/truncated_first_150.json...\u001b[0m\n","\u001b[96mLoaded 1 samples from 2WikiMultihopQA.\u001b[0m\n","\u001b[1m\u001b[96mStarting evaluation of 2WikiMultihopQA dataset...\u001b[0m\n","\n","\u001b[1m================================================================================\u001b[0m\n","\u001b[94m\u001b[1mSAMPLE 1/1 - DATASET: 2WikiMultihopQA\u001b[0m\n","\u001b[94m--------------------------------------------------------------------------------\u001b[0m\n","\u001b[92mPROMPT:\u001b[0m\n","\u001b[92m\n","\n","Question: Who is the mother of the director of film Polish-Russian War (Film)?\n","\n","Think step-by-step to answer the question based on the context.\n","\n","After your thinking, provide your final answer in a latex boxed format:\n","$\\boxed{<finalAnswer>}$\n","\u001b[0m\n","\u001b[94m--------------------------------------------------------------------------------\u001b[0m\n","\u001b[95mMODEL RESPONSE:\u001b[0m\n","\u001b[95msystem\n","\n","Cutting Knowledge Date: December 2023\n","Today Date: 11 Mar 2025\n","\n","user\n","\n","Question: Who is the mother of the director of film Polish-Russian War (Film)?\n","\n","Think step-by-step to answer the question based on the context.\n","\n","After your thinking, provide your final answer in a latex boxed format:\n","$\\boxed{<finalAnswer>}$assistant\n","\n","To find the mother of the director of the film Polish-Russian War (Film), we need to look at the information about the war's director.\n","\n","The war in question is the Film Polish-Russian War:\n","- It was directed by Andrzej Mąkiewski, a Polish filmmaker.\n","- The mother of the director of this war is Polish, as she is the one who supports and has birth to his child.\n","\n","Therefore, the mother of the director of the film Polish-Russian War (Film) is Polish.\n","$\\boxed{<Polish>$\u001b[0m\n","\u001b[94m--------------------------------------------------------------------------------\u001b[0m\n","\u001b[93mPREDICTION: <finalAnswer>\u001b[0m\n","\u001b[93mTRUE LABEL: Małgorzata Braunek\u001b[0m\n","\u001b[91mCORRECT: False\u001b[0m\n","\u001b[1m================================================================================\u001b[0m\n","\u001b[96mProcessed 1/1 samples - Avg time per sample: 2.77s - Estimated time remaining: 0.0 minutes\u001b[0m\n","\n","\u001b[1mResults for 2WikiMultihopQA:\u001b[0m\n","\u001b[1mOverall accuracy: 0.00\u001b[0m\n"]},{"output_type":"stream","name":"stderr","text":["The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":["\u001b[92mResults saved to results_2WikiMultihopQA/\u001b[0m\n","\u001b[96mLoading SVAMP dataset from /content/drive/Shareddrives/517 nlp project/data/SVAMP/test.json...\u001b[0m\n","\u001b[96mLoaded 1 samples from SVAMP.\u001b[0m\n","\u001b[1m\u001b[96mStarting evaluation of SVAMP dataset...\u001b[0m\n","\n","\u001b[1m================================================================================\u001b[0m\n","\u001b[94m\u001b[1mSAMPLE 1/1 - DATASET: SVAMP\u001b[0m\n","\u001b[94m--------------------------------------------------------------------------------\u001b[0m\n","\u001b[92mPROMPT:\u001b[0m\n","\u001b[92mProblem: Debby bought 95 soda bottles and 180 water bottles when they were on sale. If she drank 15 water bottles and 54 soda bottles a day\n","How many days would the water bottles last?\n","\n","Think step-by-step to solve this math problem.\n","\n","After your thinking, provide your final numeric answer in a latex boxed format:\n","$\\boxed{<finalAnswer>}$\n","\u001b[0m\n","\u001b[94m--------------------------------------------------------------------------------\u001b[0m\n"]},{"output_type":"stream","name":"stderr","text":["The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":["\u001b[95mMODEL RESPONSE:\u001b[0m\n","\u001b[95msystem\n","\n","Cutting Knowledge Date: December 2023\n","Today Date: 11 Mar 2025\n","\n","user\n","\n","Problem: Debby bought 95 soda bottles and 180 water bottles when they were on sale. If she drank 15 water bottles and 54 soda bottles a day\n","How many days would the water bottles last?\n","\n","Think step-by-step to solve this math problem.\n","\n","After your thinking, provide your final numeric answer in a latex boxed format:\n","$\\boxed{<finalAnswer>}$assistant\n","\n","To find out how long the water bottles last when she drinks 15 water bottles and 54 soda bottles a day, we need to first calculate the combined rate of both type of bottles.\n","\n","The difference between the number of water bottles and the number of soda bottles is 185 (180 - 15). This 185 is the combined rate at which she is drinking both type of bottles.\n","\n","Now, we need to divide the total number of water bottles (95 + 180) by the combined rate to find out how many days the water bottles last.\n","\n","(95 + 180) is the total number of water bottles she has.\n","185 is the combined rate of both type of bottles.\n","\n","To divide the total number of water bottles by the combined rate, we will use the following formula:\n","Lastability of water bottles = (Total number of water bottles / Combined rate)\n","= (95 + 180 / 185)\n","= (275 / 185)\n","= 1.5428 (She will drink the water bottles for approximately 1.54 days.\n","\n","Therefore, the water bottles will last for about 1.54 days.\u001b[0m\n","\u001b[94m--------------------------------------------------------------------------------\u001b[0m\n","\u001b[93mPREDICTION: <finalAnswer>\u001b[0m\n","\u001b[93mTRUE LABEL: 12.0\u001b[0m\n","\u001b[91mCORRECT: False\u001b[0m\n","\u001b[1m================================================================================\u001b[0m\n","\u001b[96mProcessed 1/1 samples - Avg time per sample: 6.22s - Estimated time remaining: 0.0 minutes\u001b[0m\n","\n","\u001b[1mResults for SVAMP:\u001b[0m\n","\u001b[1mOverall accuracy: 0.00\u001b[0m\n","\u001b[91mError creating visualizations for SVAMP: ufunc 'greater' did not contain a loop with signature matching types (<class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.StrDType'>) -> None\u001b[0m\n","\u001b[92mResults saved to results_SVAMP/\u001b[0m\n","\u001b[96mLoading Bamboogle dataset from /content/drive/Shareddrives/517 nlp project/data/Bamboogle/test.json...\u001b[0m\n","\u001b[96mLoaded 1 samples from Bamboogle.\u001b[0m\n","\u001b[1m\u001b[96mStarting evaluation of Bamboogle dataset...\u001b[0m\n","\n","\u001b[1m================================================================================\u001b[0m\n","\u001b[94m\u001b[1mSAMPLE 1/1 - DATASET: Bamboogle\u001b[0m\n","\u001b[94m--------------------------------------------------------------------------------\u001b[0m\n","\u001b[92mPROMPT:\u001b[0m\n","\u001b[92m\n","\n","Question: Who was president of the United States in the year that Citibank was founded?\n","\n","Think step-by-step to answer the question based on the context.\n","\n","After your thinking, provide your final answer in a latex boxed format:\n","$\\boxed{<finalAnswer>}$\n","\u001b[0m\n","\u001b[94m--------------------------------------------------------------------------------\u001b[0m\n","\u001b[95mMODEL RESPONSE:\u001b[0m\n","\u001b[95msystem\n","\n","Cutting Knowledge Date: December 2023\n","Today Date: 11 Mar 2025\n","\n","user\n","\n","Question: Who was president of the United States in the year that Citibank was founded?\n","\n","Think step-by-step to answer the question based on the context.\n","\n","After your thinking, provide your final answer in a latex boxed format:\n","$\\boxed{<finalAnswer>}$assistant\n","\n","The question requires the information about the history of Presidents and Citibank's founding.\n","\n","1. **Get the information about Presidents and Citibank's founding**:\n","- The question needs the information about the presidencies and the establishment of Citibank.\n","- \n","- For example, we know that:\n","- George Washington was the first President and Citibank was founded after that.\n","- \n","- But we do not know in which Year Washington had Citibank founded.\n","\n","Therefore, the question needs the specific information about the presidencies and Citibank's founding.\n","\n","If we think about the information step-by-step:\n","\n","- We know that:\n","- George Washington was the first President and he founded Citibank in 1852.\n","- \n","- This information is sufficient to answer the question.\n","\n","Therefore, the final answer is:\n","$\\boxed{1852}$\u001b[0m\n","\u001b[94m--------------------------------------------------------------------------------\u001b[0m\n","\u001b[93mPREDICTION: 1852\u001b[0m\n","\u001b[93mTRUE LABEL: james madison\u001b[0m\n","\u001b[91mCORRECT: False\u001b[0m\n","\u001b[1m================================================================================\u001b[0m\n","\u001b[96mProcessed 1/1 samples - Avg time per sample: 4.46s - Estimated time remaining: 0.0 minutes\u001b[0m\n","\n","\u001b[1mResults for Bamboogle:\u001b[0m\n","\u001b[1mOverall accuracy: 0.00\u001b[0m\n","\u001b[92mResults saved to results_Bamboogle/\u001b[0m\n"]}],"source":["# Choose which datasets to evaluate\n","datasets_to_evaluate = [\n","    \"VitaminC\",\n","    \"FEVER\",\n","    \"FEVEROUS\",\n","    \"HotpotQA\",\n","    \"2WikiMultihopQA\",\n","    \"SVAMP\",\n","    \"Bamboogle\"\n","]  # Evaluate all available datasets\n","\n","# Initialize results summary\n","summary = {}\n","\n","# Process each selected dataset\n","for dataset_name in datasets_to_evaluate:\n","    if dataset_name not in datasets:\n","        print(f\"{RED}Dataset {dataset_name} not found in available datasets{ENDC}\")\n","        continue\n","\n","    # Load dataset\n","    file_path = datasets[dataset_name]\n","    data = load_dataset(file_path, dataset_name, limit=1)  # Limit to 1 sample\n","\n","    if data:\n","        # Evaluate on the dataset\n","        accuracy, _ = evaluate_dataset(model, tokenizer, data, dataset_name)\n","        summary[dataset_name] = accuracy"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Bakui531yHwn","colab":{"base_uri":"https://localhost:8080/","output_embedded_package_id":"1MqM4F_pQ-eDixb-Ea4fFweRkGAtyThBe"},"outputId":"b9f25f9f-4e6e-4449-ae9b-ffea355e6481"},"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}],"source":["# Choose which datasets to evaluate\n","# You can modify this list to evaluate specific datasets\n","# datasets_to_evaluate = [\"VitaminC\", \"FEVER\"]  # Uncomment to evaluate specific datasets\n","datasets_to_evaluate = [\n","    \"VitaminC\",\n","    # \"FEVER\",\n","    # \"FEVEROUS\",\n","    \"HotpotQA\",\n","    \"2WikiMultihopQA\",\n","    \"SVAMP\",\n","    \"Bamboogle\"\n","]  # Evaluate all available datasets\n","# Initialize results summary\n","summary = {}\n","\n","# Process each selected dataset\n","for dataset_name in datasets_to_evaluate:\n","    if dataset_name not in datasets:\n","        print(f\"{RED}Dataset {dataset_name} not found in available datasets{ENDC}\")\n","        continue\n","\n","    # Load dataset\n","    file_path = datasets[dataset_name]\n","    data = load_dataset(file_path, dataset_name, limit=150)  # Limit to 150 samples\n","\n","    if data:\n","        # Evaluate on the dataset\n","        accuracy, _ = evaluate_dataset(model, tokenizer, data, dataset_name)\n","        summary[dataset_name] = accuracy"]},{"cell_type":"markdown","metadata":{"id":"QPgP1zLsyHwn"},"source":["## Results Visualization\n","\n","Create summary visualizations of model performance across datasets"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BOOg_Y7OyHwn"},"outputs":[],"source":["# Print summary of results\n","print(f\"\\n{BOLD}{CYAN}Summary of Results:{ENDC}\")\n","for dataset, accuracy in summary.items():\n","    print(f\"{dataset}: {accuracy:.4f}\")\n","\n","# Create summary visualization\n","if summary:\n","    plt.figure(figsize=(12, 6))\n","    datasets = list(summary.keys())\n","    accuracies = list(summary.values())\n","\n","    # Sort by accuracy\n","    sorted_indices = np.argsort(accuracies)[::-1]\n","    sorted_datasets = [datasets[i] for i in sorted_indices]\n","    sorted_accuracies = [accuracies[i] for i in sorted_indices]\n","\n","    bars = plt.bar(sorted_datasets, sorted_accuracies)\n","\n","    # Add value labels on top of bars\n","    for bar in bars:\n","        height = bar.get_height()\n","        plt.text(bar.get_x() + bar.get_width()/2., height + 0.01,\n","                f'{height:.4f}', ha='center', va='bottom')\n","\n","    plt.ylim(0, 1.1)  # Set y limit to 0-1 with a small margin\n","    plt.xlabel('Datasets')\n","    plt.ylabel('Accuracy')\n","    plt.title('Llama-3.2-1B Performance Across Datasets')\n","    plt.xticks(rotation=45)\n","    plt.tight_layout()\n","    plt.savefig(\"summary_results.png\")\n","    plt.show()\n","\n","    # Save summary to CSV\n","    summary_df = pd.DataFrame(list(summary.items()), columns=['Dataset', 'Accuracy'])\n","    summary_df = summary_df.sort_values('Accuracy', ascending=False)\n","    summary_df.to_csv(\"summary_results.csv\", index=False)\n","\n","    print(f\"\\n{GREEN}Summary saved to summary_results.csv and summary_results.png{ENDC}\")\n","else:\n","    print(f\"\\n{RED}No results to visualize{ENDC}\")"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[{"file_id":"1jwqZ8_GIFulvhB5nYxdBFewP9Nt1-alY","timestamp":1741677215000}]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"oAZSmoxu7Rhw","executionInfo":{"status":"ok","timestamp":1740718885810,"user_tz":480,"elapsed":18475,"user":{"displayName":"Jeremy Huang","userId":"12545693065398382142"}},"outputId":"93e64d2d-ac31-47eb-c6cb-9d0a7567473a","colab":{"base_uri":"https://localhost:8080/"}},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RPMMsxDg7Rhy"},"outputs":[],"source":["# Load required libraries\n","from transformers import AutoModelForCausalLM, AutoTokenizer\n","import pandas as pd\n","import numpy as np\n","import time\n","import re"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AYf3U9c17Rhy"},"outputs":[],"source":["# Load model and tokenizer\n","load_directory = \"PATH_TO_YOUR_MODEL\"  # Update this path\n","tokenizer = AutoTokenizer.from_pretrained(load_directory)\n","model = AutoModelForCausalLM.from_pretrained(load_directory).to(\"cuda\")\n","\n","# Ensure the tokenizer has a pad_token\n","if tokenizer.pad_token is None:\n","    tokenizer.add_special_tokens({'pad_token': tokenizer.eos_token})\n","    model.resize_token_embeddings(len(tokenizer))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"u20LdJu37Rhz"},"outputs":[],"source":["def generate_math_prompt(question):\n","    prompt = f\"\"\"Solve this math word problem step by step:\n","\n","{question}\n","\n","Let's solve this step by step:\n","1) First, understand what is being asked\n","2) Then, identify the important information\n","3) Solve step by step\n","4) Finally, state the answer in the format: [ANSWER]<number>[/ANSWER]\n","\n","Show your work:\"\"\"\n","    return prompt\n","\n","def extract_answer(response):\n","    \"\"\"Extract the numerical answer from between [ANSWER] tags\"\"\"\n","    try:\n","        # Look for answer between tags\n","        pattern = r'\\[ANSWER\\](\\d+\\.?\\d*)\\[\\/ANSWER\\]'\n","        match = re.search(pattern, response)\n","        if match:\n","            return float(match.group(1))\n","\n","        # Fallback: look for last number after \"answer is\" or similar phrases\n","        patterns = [\n","            r'answer is[^\\d]*(\\d+\\.?\\d*)',\n","            r'answer:[^\\d]*(\\d+\\.?\\d*)',\n","            r'equals[^\\d]*(\\d+\\.?\\d*)',\n","            r'=\\s*(\\d+\\.?\\d*)'\n","        ]\n","\n","        for pattern in patterns:\n","            match = re.search(pattern, response.lower())\n","            if match:\n","                return float(match.group(1))\n","\n","        # Final fallback: last number in response\n","        numbers = re.findall(r'\\d+\\.?\\d*', response)\n","        if numbers:\n","            return float(numbers[-1])\n","\n","        return None\n","    except:\n","        return None"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cecWvIKf7Rhz"},"outputs":[],"source":["# Load evaluation dataset\n","dataset_path = \"PATH_TO_YOUR_EVAL_DATASET\"  # Update this path\n","df = pd.read_csv(dataset_path)\n","\n","# Format questions\n","def format_question(row):\n","    numbers = row['Numbers'].split()\n","    body = row['Body']\n","    question = row['Ques']\n","\n","    for i, num in enumerate(numbers):\n","        body = body.replace(f'number{i}', num)\n","        question = question.replace(f'number{i}', num)\n","\n","    return f\"Problem: {body}\\nQuestion: {question}\"\n","\n","df['formatted_question'] = df.apply(format_question, axis=1)\n","print(f\"Total evaluation examples: {len(df)}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uAZgqjLu7Rhz"},"outputs":[],"source":["# Run evaluation\n","predictions = []\n","actual = []\n","start_time = time.time()\n","\n","# ANSI color codes for output formatting\n","BLUE = '\\033[94m'\n","GREEN = '\\033[92m'\n","RED = '\\033[91m'\n","YELLOW = '\\033[93m'\n","CYAN = '\\033[96m'\n","PURPLE = '\\033[95m'\n","ENDC = '\\033[0m'\n","\n","total_questions = len(df)\n","\n","for idx, row in df.iterrows():\n","    question = row['formatted_question']\n","    prompt = generate_math_prompt(question)\n","\n","    # Generate response\n","    inputs = tokenizer(prompt, return_tensors=\"pt\", padding=True).to(\"cuda\")\n","    output = model.generate(\n","        inputs.input_ids,\n","        max_length=512,\n","        temperature=0.1,\n","        do_sample=True,\n","        num_beams=3,\n","        top_p=0.9,\n","        repetition_penalty=1.2\n","    )\n","    response = tokenizer.decode(output[0], skip_special_tokens=True)\n","\n","    # Extract and evaluate answer\n","    predicted = extract_answer(response)\n","    if predicted is not None:\n","        predictions.append(predicted)\n","        actual.append(float(row['Answer']))\n","\n","        is_correct = abs(predicted - float(row['Answer'])) < 0.01\n","        correct_color = GREEN if is_correct else RED\n","\n","        # Print progress\n","        questions_done = len(predictions)\n","        questions_left = total_questions - questions_done\n","        elapsed_time = time.time() - start_time\n","        avg_time_per_question = elapsed_time / questions_done if questions_done > 0 else 0\n","        estimated_time_left = questions_left * avg_time_per_question\n","\n","        print(f\"\\n{CYAN}Progress: {questions_done}/{total_questions} questions{ENDC}\")\n","        print(f\"{CYAN}Average time per question: {avg_time_per_question:.1f} seconds{ENDC}\")\n","        print(f\"{CYAN}Estimated time remaining: {estimated_time_left/60:.1f} minutes{ENDC}\\n\")\n","\n","        # Print results\n","        print(f\"\\n{YELLOW}Question {idx}:{ENDC}\")\n","        print(f\"{BLUE}Prompt: {question}{ENDC}\")\n","        print(f\"{YELLOW}Model response:{ENDC}\")\n","        print(f\"{PURPLE}{response}{ENDC}\")\n","        print(f\"{correct_color}Correct: {is_correct}{ENDC}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nHBKEFe07Rhz"},"outputs":[],"source":["# Calculate and display metrics\n","predictions = np.array(predictions)\n","actual = np.array(actual)\n","errors = np.abs(predictions - actual)\n","\n","print(\"=== Evaluation Metrics ===\")\n","print(f\"Total examples evaluated: {len(predictions)}\")\n","print(f\"Mean absolute error: {errors.mean():.2f}\")\n","print(f\"Median absolute error: {np.median(errors):.2f}\")\n","print(f\"Exact matches: {(errors < 0.01).sum()}/{len(predictions)} ({(errors < 0.01).mean()*100:.1f}%)\")"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
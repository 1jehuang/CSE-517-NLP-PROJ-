{"cells":[{"cell_type":"markdown","metadata":{"id":"zl1EikjgyHwi"},"source":["# Evaluating Llama-3.2-1B on Multiple Datasets\n","\n","This notebook evaluates the Llama-3.2-1B model on multiple datasets:\n","- **VitaminC**: Fact verification\n","- **FEVER/FEVEROUS**: Fact verification\n","- **HotpotQA/2WikiMultihopQA**: Multi-hop question answering\n","- **SVAMP**: Math word problems\n","- **Bamboogle**: General question answering\n","\n","The evaluation uses dataset-specific prompting strategies and metrics."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_rseoOajyHwi","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1741501678725,"user_tz":480,"elapsed":58447,"user":{"displayName":"Jeremy Huang","userId":"00765697131879146443"}},"outputId":"8cb3f1b7-0533-4cfe-ad9d-5a7ba1340040"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["import json\n","import time\n","import re\n","import random\n","import os\n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","from google.colab import drive\n","from transformers import AutoModelForCausalLM, AutoTokenizer\n","from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n","import torch\n","import math\n","\n","# ANSI color codes for terminal output\n","BLUE = '\\033[94m'     # For sample information\n","GREEN = '\\033[92m'    # For correct predictions and success messages\n","RED = '\\033[91m'      # For incorrect predictions\n","YELLOW = '\\033[93m'   # For predictions and headers\n","CYAN = '\\033[96m'     # For progress information\n","PURPLE = '\\033[95m'   # For model responses\n","BOLD = '\\033[1m'      # Bold text\n","ENDC = '\\033[0m'      # End color\n","\n","# Mount Google Drive\n","drive.mount('/content/drive')"]},{"cell_type":"markdown","metadata":{"id":"EkF1m0HgyHwj"},"source":["## Dataset Loading Functions\n","\n","Functions to load different dataset formats (JSON and JSONL)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2X_wwVyoyHwj"},"outputs":[],"source":["def load_dataset(file_path, dataset_name, limit=150):\n","    \"\"\"Load samples from various datasets with appropriate format handling\"\"\"\n","    print(f\"{CYAN}Loading {dataset_name} dataset from {file_path}...{ENDC}\")\n","    data = []\n","\n","    if not os.path.exists(file_path):\n","        print(f\"{RED}File not found: {file_path}{ENDC}\")\n","        return []\n","\n","    if file_path.endswith('.jsonl'):\n","        with open(file_path, 'r', encoding='utf-8') as f:\n","            for line in f:\n","                if line.strip():  # Skip empty lines\n","                    data.append(json.loads(line))\n","                    if len(data) >= limit:\n","                        break\n","    else:  # .json files\n","        with open(file_path, 'r', encoding='utf-8') as f:\n","            json_data = json.load(f)\n","            if isinstance(json_data, list):\n","                data = json_data[:limit]\n","            else:\n","                # Handle nested structures if needed\n","                if 'data' in json_data:\n","                    data = json_data['data'][:limit]\n","                else:\n","                    print(f\"{YELLOW}Warning: Unexpected JSON structure in {file_path}{ENDC}\")\n","                    data = [json_data]  # Just use the whole object as one sample\n","\n","    print(f\"{CYAN}Loaded {len(data)} samples from {dataset_name}.{ENDC}\")\n","    return data"]},{"cell_type":"markdown","metadata":{"id":"9vaoNO3IyHwj"},"source":["## Prompt Creation Functions\n","\n","Different prompting strategies for each dataset type"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9VGJ7wQmyHwk"},"outputs":[],"source":["def create_prompt(sample, dataset_name):\n","    \"\"\"Create appropriate prompts based on dataset type\"\"\"\n","\n","    if dataset_name == \"VitaminC\" or dataset_name == \"FEVER\" or dataset_name == \"FEVEROUS\":\n","        # Fact verification datasets\n","        if dataset_name == \"VitaminC\":\n","            claim = sample[\"claim\"]\n","            evidence = sample[\"evidence\"]\n","        elif dataset_name == \"FEVER\":\n","            claim = sample[\"claim\"]\n","            evidence = sample.get(\"evidence\", \"\") or sample.get(\"context\", \"\")\n","        elif dataset_name == \"FEVEROUS\":\n","            claim = sample[\"claim\"]\n","            evidence = sample.get(\"evidence\", \"\") or sample.get(\"context\", \"\")\n","\n","        return create_fact_verification_prompt(claim, evidence)\n","\n","    elif dataset_name == \"HotpotQA\" or dataset_name == \"2WikiMultihopQA\":\n","        # Multi-hop QA datasets\n","        question = sample.get(\"question\", \"\") or sample.get(\"query\", \"\")\n","        context = sample.get(\"context\", \"\")\n","        if not context and \"original_context\" in sample:\n","            context = sample[\"original_context\"]\n","\n","        return create_qa_prompt(question, context)\n","\n","    elif dataset_name == \"SVAMP\":\n","        # Math word problem dataset\n","        question = sample.get(\"question\", \"\") or sample.get(\"body\", \"\")\n","        return create_math_prompt(question)\n","\n","    elif dataset_name == \"Bamboogle\":\n","        # Bamboogle dataset - assume it's a QA task\n","        question = sample.get(\"question\", \"\")\n","        context = sample.get(\"context\", \"\")\n","\n","        if not question and \"answer\" in sample:\n","            # If no question is provided but there's an answer, create a generic prompt\n","            return\n","\n","        return create_qa_prompt(question, context)\n","\n","    else:\n","        # Generic prompt for unknown datasets\n","        return f\"Please analyze this data and think step-by-step:\\n\\n{json.dumps(sample, indent=2)}\\n\\nAfter your thinking, provide your answer in a latex boxed format:\\n$\\\\boxed{{<finalAnswer>}}$\"\n","\n","def create_fact_verification_prompt(claim, evidence):\n","    \"\"\"Create a prompt for fact checking\"\"\"\n","    prompt = f\"\"\"Claim: {claim}\n","\n","Evidence: {evidence}\n","\n","Think step-by-step to determine if the evidence SUPPORTS, REFUTES, or provides NOT ENOUGH INFO for the claim.\n","\n","After your thinking, end your answer with one of these in a latex boxed format:\n","$\\\\boxed{{SUPPORTS}}$ or $\\\\boxed{{REFUTES}}$ or $\\\\boxed{{NOT ENOUGH INFO}}$\n","\n","Always end with your answer in a latex boxed format:\n","$\\\\boxed{{<finalAnswer>}}$\"\"\"\n","    return prompt\n","\n","def create_qa_prompt(question, context):\n","    \"\"\"Create a prompt for question answering tasks\"\"\"\n","    prompt = f\"\"\"\n","\n","Question: {question}\n","\n","Think step-by-step to answer the question based on the context.\n","\n","After your thinking, provide your final answer in a latex boxed format:\n","$\\\\boxed{{<finalAnswer>}}$\n","\"\"\"\n","    return prompt\n","\n","def create_math_prompt(question):\n","    \"\"\"Create a prompt for math word problems\"\"\"\n","    prompt = f\"\"\"Problem: {question}\n","\n","Think step-by-step to solve this math problem.\n","\n","After your thinking, provide your final numeric answer in a latex boxed format:\n","$\\\\boxed{{<finalAnswer>}}$\n","\"\"\"\n","    return prompt\n"]},{"cell_type":"code","source":["print( create_math_prompt(\"hi\"))"],"metadata":{"id":"gfX_gtYZZ6Da","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1741501678834,"user_tz":480,"elapsed":4,"user":{"displayName":"Jeremy Huang","userId":"00765697131879146443"}},"outputId":"b9964fdc-5f58-4425-e065-17c318c65c95"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Problem: hi\n","\n","Think step-by-step to solve this math problem.\n","\n","After your thinking, provide your final numeric answer in a latex boxed format:\n","$\\boxed{<finalAnswer>}$\n","\n"]}]},{"cell_type":"markdown","metadata":{"id":"IMgkydTGyHwk"},"source":["## Model Inference Function"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aYwi1s50yHwl"},"outputs":[],"source":["def run_inference(model, tokenizer, prompt, device=\"cuda\"):\n","    \"\"\"Run inference on the model\"\"\"\n","    # Tokenize input with attention mask\n","    encoding = tokenizer(prompt, return_tensors=\"pt\", padding=True, truncation=True, max_length=128000)\n","\n","    inputs = {\n","        'input_ids': encoding.input_ids.to(device),\n","        'attention_mask': encoding.attention_mask.to(device)\n","    }\n","\n","    with torch.no_grad():\n","        outputs = model.generate(\n","            input_ids=inputs['input_ids'],\n","            attention_mask=inputs['attention_mask'],\n","            max_new_tokens=2048,  # Adjust based on desired output length\n","            temperature=0.2,      # Lower temperature for focused output\n","            do_sample=True,\n","        )\n","\n","    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n"]},{"cell_type":"markdown","metadata":{"id":"ARcR_SomyHwl"},"source":["## Prediction Extraction Functions\n","\n","Functions to extract structured predictions from model outputs"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gp0jtmxWyHwl","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1741501678938,"user_tz":480,"elapsed":19,"user":{"displayName":"Jeremy Huang","userId":"00765697131879146443"}},"outputId":"87485b15-0f8b-4039-f126-201704950826"},"outputs":[{"output_type":"stream","name":"stdout","text":["Test: Fraction inside box - expecting \\frac{1}{2}\n","  Dataset: RandomDataset\n","  Output: No recognized dataset... final is $\\boxed{\\frac{1}{2}}$\n","  Expected: '\\\\\\\\frac{1}{2}', Got: '\\\\frac{1'\n","  => FAILED\n","\n","Test: No box at all (should return None)\n","  Dataset: VitaminC\n","  Output: This text does not contain any \\boxed{} expression.\n","  Expected: None, Got: None\n","  => PASSED\n","\n","Test: Box with empty content (should return None)\n","  Dataset: SVAMP\n","  Output: Check \\boxed{} here. That's all.\n","  Expected: None, Got: None\n","  => PASSED\n","\n","Test: Multiple boxes, last is fraction\n","  Dataset: HotpotQA\n","  Output: Mid: $\\boxed{SUPPORTS}$ Then final: $\\boxed{\\frac{2}{5}}$\n","  Expected: '\\\\\\\\frac{2}{5}', Got: '\\\\frac{2'\n","  => FAILED\n","\n","Passed 2/4 tests.\n"]}],"source":["import re\n","\n","def extract_prediction(output, dataset_name, sample=None):\n","    \"\"\"\n","    Extract the final boxed answer for any dataset.\n","    If we only find an empty \\boxed{}, treat it as no valid box.\n","    \"\"\"\n","    # For now, ignoring dataset_name / sample, just returning the last \\boxed{...} if not empty\n","    pattern = r'\\\\boxed\\s*\\{([^}]*)\\}'\n","    all_matches = re.findall(pattern, output)\n","\n","    # Filter out empty or whitespace-only matches\n","    non_empty_matches = [m.strip() for m in all_matches if m.strip()]\n","\n","    # If no valid non-empty box found, return None\n","    if not non_empty_matches:\n","        return None\n","\n","    # Return the last valid boxed expression\n","    return non_empty_matches[-1]\n","\n","\n","def run_tests():\n","    test_cases = [\n","        {\n","            \"name\": \"Fraction inside box - expecting \\\\frac{1}{2}\",\n","            \"dataset\": \"RandomDataset\",\n","            \"output\": r\"No recognized dataset... final is $\\boxed{\\frac{1}{2}}$\",\n","            \"expected\": r\"\\\\frac{1}{2}\"\n","        },\n","        {\n","            \"name\": \"No box at all (should return None)\",\n","            \"dataset\": \"VitaminC\",\n","            \"output\": r\"This text does not contain any \\boxed{} expression.\",\n","            \"expected\": None\n","        },\n","        {\n","            \"name\": \"Box with empty content (should return None)\",\n","            \"dataset\": \"SVAMP\",\n","            \"output\": r\"Check \\boxed{} here. That's all.\",\n","            \"expected\": None\n","        },\n","        {\n","            \"name\": \"Multiple boxes, last is fraction\",\n","            \"dataset\": \"HotpotQA\",\n","            \"output\": (\n","                r\"Mid: $\\boxed{SUPPORTS}$ \"\n","                r\"Then final: $\\boxed{\\frac{2}{5}}$\"\n","            ),\n","            \"expected\": r\"\\\\frac{2}{5}\"\n","        },\n","    ]\n","\n","    passes = 0\n","    total = len(test_cases)\n","\n","    for t in test_cases:\n","        pred = extract_prediction(t[\"output\"], t[\"dataset\"])\n","        success = (pred == t[\"expected\"])\n","        if success:\n","            passes += 1\n","\n","        print(f\"Test: {t['name']}\")\n","        print(f\"  Dataset: {t['dataset']}\")\n","        print(f\"  Output: {t['output']}\")\n","        print(f\"  Expected: {t['expected']!r}, Got: {pred!r}\")\n","        print(f\"  => {'PASSED' if success else 'FAILED'}\\n\")\n","\n","    print(f\"Passed {passes}/{total} tests.\")\n","\n","\n","if __name__ == \"__main__\":\n","    run_tests()\n"]},{"cell_type":"markdown","metadata":{"id":"iBl6VmcEyHwm"},"source":["## Evaluation Metrics Functions\n","\n","Functions to compare predictions with ground truth"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aMwHp_1JyHwm"},"outputs":[],"source":["def evaluate_correctness(prediction, ground_truth, dataset_name):\n","    \"\"\"Evaluate if the prediction is correct based on dataset type\"\"\"\n","\n","    if dataset_name == \"VitaminC\" or dataset_name == \"FEVER\" or dataset_name == \"FEVEROUS\":\n","        # Fact verification - direct comparison\n","        return prediction == ground_truth\n","\n","    elif dataset_name == \"HotpotQA\" or dataset_name == \"2WikiMultihopQA\" or dataset_name == \"Bamboogle\":\n","        # QA evaluation - normalize and compare\n","        return normalize_qa_answers(prediction, ground_truth)\n","\n","    elif dataset_name == \"SVAMP\":\n","        # Math evaluation - numeric comparison\n","        return evaluate_math_correctness(prediction, ground_truth)\n","\n","    else:\n","        # Generic comparison for unknown datasets\n","        return prediction == ground_truth\n","\n","def normalize_qa_answers(prediction, ground_truth):\n","    \"\"\"Normalize and compare QA answers with flexible matching\"\"\"\n","    if not prediction or not ground_truth:\n","        return False\n","\n","    # Handle list or dictionary ground truths\n","    if isinstance(ground_truth, list):\n","        ground_truth = \" \".join([str(item) for item in ground_truth])\n","    elif isinstance(ground_truth, dict):\n","        if \"answer\" in ground_truth:\n","            ground_truth = ground_truth[\"answer\"]\n","        else:\n","            ground_truth = str(ground_truth)\n","\n","    # Normalize both strings\n","    pred_norm = prediction.lower().strip()\n","    truth_norm = str(ground_truth).lower().strip()\n","\n","    # Remove punctuation and extra spaces\n","    pred_norm = re.sub(r'[^\\w\\s]', '', pred_norm).strip()\n","    truth_norm = re.sub(r'[^\\w\\s]', '', truth_norm).strip()\n","\n","    # Check if prediction contains ground truth or vice versa\n","    return pred_norm in truth_norm or truth_norm in pred_norm\n","\n","def evaluate_math_correctness(prediction, ground_truth):\n","    \"\"\"Evaluate correctness of math answers with tolerance\"\"\"\n","    try:\n","        # Convert to numeric values\n","        if isinstance(prediction, str):\n","            prediction = float(re.search(r'(-?[\\d.]+)', prediction.replace(',', '')).group(1))\n","\n","        if isinstance(ground_truth, str):\n","            ground_truth = float(re.search(r'(-?[\\d.]+)', ground_truth.replace(',', '')).group(1))\n","\n","        # Compare with tolerance\n","        tolerance = 0.01\n","        return abs(float(prediction) - float(ground_truth)) < tolerance\n","    except (ValueError, TypeError, AttributeError):\n","        return False\n","\n","def get_ground_truth(sample, dataset_name):\n","    \"\"\"Extract ground truth from sample based on dataset type\"\"\"\n","\n","    if dataset_name == \"VitaminC\":\n","        return sample.get(\"label\", \"\")\n","\n","    elif dataset_name == \"FEVER\" or dataset_name == \"FEVEROUS\":\n","        return sample.get(\"label\", \"\")\n","\n","    elif dataset_name == \"HotpotQA\" or dataset_name == \"2WikiMultihopQA\":\n","        return sample.get(\"answer\", \"\")\n","\n","    elif dataset_name == \"SVAMP\":\n","        return sample.get(\"answer\", None)\n","\n","    elif dataset_name == \"Bamboogle\":\n","        return sample.get(\"answer\", \"\")\n","\n","    else:\n","        return None"]},{"cell_type":"markdown","metadata":{"id":"XxhWQHyQyHwm"},"source":["## Visualization Functions\n","\n","Functions to create visualizations of results"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ym19hvtvyHwm"},"outputs":[],"source":["def visualize_results(results_df, dataset_name):\n","    \"\"\"Create visualizations of results for a specific dataset\"\"\"\n","    output_dir = f\"results_{dataset_name}\"\n","    os.makedirs(output_dir, exist_ok=True)\n","\n","    if 'true_label' in results_df.columns and 'prediction' in results_df.columns:\n","        # Fact verification datasets\n","        if len(results_df) > 0:\n","            try:\n","                labels = sorted(list(set(results_df['true_label'].unique()).union(set(results_df['prediction'].unique()))))\n","\n","                # Create confusion matrix\n","                cm = confusion_matrix(\n","                    results_df['true_label'],\n","                    results_df['prediction'],\n","                    labels=labels\n","                )\n","\n","                plt.figure(figsize=(10, 8))\n","                sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=labels, yticklabels=labels)\n","                plt.xlabel('Predicted')\n","                plt.ylabel('True')\n","                plt.title(f'Confusion Matrix - {dataset_name}')\n","                plt.tight_layout()\n","                plt.savefig(f\"{output_dir}/confusion_matrix.png\")\n","                plt.close()\n","\n","                # Class-wise accuracy\n","                class_accuracy = {}\n","                for cls in labels:\n","                    cls_indices = results_df['true_label'] == cls\n","                    if cls_indices.any():\n","                        correct = (results_df.loc[cls_indices, 'prediction'] == cls).sum()\n","                        class_accuracy[cls] = correct / cls_indices.sum()\n","\n","                plt.figure(figsize=(10, 6))\n","                sns.barplot(x=list(class_accuracy.keys()), y=list(class_accuracy.values()))\n","                plt.ylim(0, 1)\n","                plt.title(f'Class-wise Accuracy - {dataset_name}')\n","                plt.ylabel('Accuracy')\n","                plt.tight_layout()\n","                plt.savefig(f\"{output_dir}/class_accuracy.png\")\n","                plt.close()\n","            except Exception as e:\n","                print(f\"{RED}Error creating visualizations for {dataset_name}: {e}{ENDC}\")\n","\n","    # Overall accuracy\n","    plt.figure(figsize=(6, 4))\n","    accuracy = results_df['correct'].mean()\n","    plt.bar(['Accuracy'], [accuracy])\n","    plt.ylim(0, 1)\n","    plt.title(f'Overall Accuracy - {dataset_name}')\n","    plt.tight_layout()\n","    plt.savefig(f\"{output_dir}/overall_accuracy.png\")\n","    plt.close()\n","\n","    # Save sample correct/incorrect examples\n","    correct_samples = results_df[results_df['correct']].head(3)\n","    incorrect_samples = results_df[~results_df['correct']].head(3)\n","\n","    with open(f\"{output_dir}/sample_results.txt\", 'w') as f:\n","        f.write(f\"=== Sample Correct Examples ({dataset_name}) ===\\n\\n\")\n","        for _, sample in correct_samples.iterrows():\n","            f.write(f\"Input: {sample.get('input', '')}\\n\")\n","            f.write(f\"True: {sample.get('true_label', '')}\\n\")\n","            f.write(f\"Prediction: {sample.get('prediction', '')}\\n\")\n","            f.write(\"\\n---\\n\\n\")\n","\n","        f.write(f\"=== Sample Incorrect Examples ({dataset_name}) ===\\n\\n\")\n","        for _, sample in incorrect_samples.iterrows():\n","            f.write(f\"Input: {sample.get('input', '')}\\n\")\n","            f.write(f\"True: {sample.get('true_label', '')}\\n\")\n","            f.write(f\"Prediction: {sample.get('prediction', '')}\\n\")\n","            f.write(\"\\n---\\n\\n\")"]},{"cell_type":"markdown","metadata":{"id":"rnAbpoz2yHwm"},"source":["## Dataset Evaluation Function\n","\n","Function to evaluate model on each dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8DkuEAkLyHwm"},"outputs":[],"source":["def evaluate_dataset(model, tokenizer, data, dataset_name):\n","    \"\"\"Evaluate model on a specific dataset\"\"\"\n","    print(f\"{BOLD}{CYAN}Starting evaluation of {dataset_name} dataset...{ENDC}\")\n","\n","    results = []\n","    start_time = time.time()\n","\n","    for i, sample in enumerate(data):\n","        # Create appropriate prompt\n","        prompt = create_prompt(sample, dataset_name)\n","\n","        # Print the prompt being sent to the model\n","        print(f\"\\n{BOLD}{'='*80}{ENDC}\")\n","        print(f\"{BLUE}{BOLD}SAMPLE {i+1}/{len(data)} - DATASET: {dataset_name}{ENDC}\")\n","        print(f\"{BLUE}{'-'*80}{ENDC}\")\n","        print(f\"{GREEN}PROMPT:{ENDC}\")\n","        print(f\"{GREEN}{prompt}{ENDC}\")\n","        print(f\"{BLUE}{'-'*80}{ENDC}\")\n","\n","        # Run inference\n","        output = run_inference(model, tokenizer, prompt)\n","\n","        # Print the model's response\n","        print(f\"{PURPLE}MODEL RESPONSE:{ENDC}\")\n","        print(f\"{PURPLE}{output}{ENDC}\")\n","\n","        # Extract prediction and ground truth\n","        prediction = extract_prediction(output, dataset_name, sample)\n","        true_label = get_ground_truth(sample, dataset_name)\n","\n","        # Evaluate correctness\n","        correct = evaluate_correctness(prediction, true_label, dataset_name)\n","        correct_color = GREEN if correct else RED\n","\n","        print(f\"{BLUE}{'-'*80}{ENDC}\")\n","        print(f\"{YELLOW}PREDICTION: {prediction}{ENDC}\")\n","        print(f\"{YELLOW}TRUE LABEL: {true_label}{ENDC}\")\n","        print(f\"{correct_color}CORRECT: {correct}{ENDC}\")\n","        print(f\"{BOLD}{'='*80}{ENDC}\")\n","\n","        # Store result\n","        result = {\n","            \"input\": prompt,\n","            \"output\": output,\n","            \"prediction\": prediction,\n","            \"true_label\": true_label,\n","            \"correct\": correct\n","        }\n","        results.append(result)\n","\n","        # Print progress\n","        if (i+1) % 10 == 0 or i == 0:\n","            elapsed = time.time() - start_time\n","            avg_time = elapsed / (i+1)\n","            remaining = avg_time * (len(data) - i - 1)\n","            print(f\"{CYAN}Processed {i+1}/{len(data)} samples - \"\n","                  f\"Avg time per sample: {avg_time:.2f}s - \"\n","                  f\"Estimated time remaining: {remaining/60:.1f} minutes{ENDC}\")\n","\n","    # Calculate overall accuracy\n","    results_df = pd.DataFrame(results)\n","    accuracy = results_df['correct'].mean()\n","\n","    print(f\"\\n{BOLD}Results for {dataset_name}:{ENDC}\")\n","    print(f\"{BOLD}Overall accuracy: {accuracy:.2f}{ENDC}\")\n","\n","    # Save results\n","    output_dir = f\"results_{dataset_name}\"\n","    os.makedirs(output_dir, exist_ok=True)\n","    results_df.to_csv(f\"{output_dir}/results.csv\", index=False)\n","\n","    # Create visualizations\n","    visualize_results(results_df, dataset_name)\n","\n","    print(f\"{GREEN}Results saved to {output_dir}/{ENDC}\")\n","\n","    return accuracy, results_df"]},{"cell_type":"markdown","metadata":{"id":"Gu9AqaTQyHwm"},"source":["## Model Loading and Dataset Evaluation\n","\n","Load the model and evaluate it on all datasets"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xSTz8tcbyHwm","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1741501744813,"user_tz":480,"elapsed":65763,"user":{"displayName":"Jeremy Huang","userId":"00765697131879146443"}},"outputId":"ca305629-8c1e-4216-da2c-9e32a4424154"},"outputs":[{"output_type":"stream","name":"stdout","text":["Loading model and tokenizer...\n","\u001b[92mModel loaded successfully\u001b[0m\n"]}],"source":["# Load model and tokenizer\n","print(\"Loading model and tokenizer...\")\n","model_path = \"/content/drive/Shareddrives/517 nlp project/Models/Llama-3.2-1B-Instruct\"\n","tokenizer = AutoTokenizer.from_pretrained(model_path)\n","model = AutoModelForCausalLM.from_pretrained(model_path).to(\"cuda\")\n","\n","# Ensure the tokenizer has a pad_token\n","if tokenizer.pad_token is None:\n","    tokenizer.add_special_tokens({'pad_token': tokenizer.eos_token})\n","    model.resize_token_embeddings(len(tokenizer))\n","\n","print(f\"{GREEN}Model loaded successfully{ENDC}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HW10Agy7yHwm","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1741501744814,"user_tz":480,"elapsed":9,"user":{"displayName":"Jeremy Huang","userId":"00765697131879146443"}},"outputId":"e4ab1976-46af-457a-d03f-87641533e7d4"},"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[96mAvailable datasets for evaluation:\u001b[0m\n","1. VitaminC\n","2. 2WikiMultihopQA\n","3. Bamboogle\n","4. FEVER\n","5. FEVEROUS\n","6. HotpotQA\n","7. SVAMP\n"]}],"source":["# Define dataset paths\n","datasets = {\n","    \"VitaminC\": \"/content/drive/Shareddrives/517 nlp project/data/VitaminC/vitaminc/test.jsonl\",\n","    \"2WikiMultihopQA\": \"/content/drive/Shareddrives/517 nlp project/data/2WikiMultihopQA/truncated_first_150.json\",\n","    \"Bamboogle\": \"/content/drive/Shareddrives/517 nlp project/data/Bamboogle/test.json\",\n","    \"FEVER\": \"/content/drive/Shareddrives/517 nlp project/data/FEVER/fever_test.jsonl\",\n","    \"FEVEROUS\": \"/content/drive/Shareddrives/517 nlp project/data/FEVEROUS/feverous_test.jsonl\",\n","    \"HotpotQA\": \"/content/drive/Shareddrives/517 nlp project/data/HotpotQA/truncated_first_150.json\",\n","    \"SVAMP\": \"/content/drive/Shareddrives/517 nlp project/data/SVAMP/test.json\"\n","}\n","\n","# Display available datasets\n","print(f\"{CYAN}Available datasets for evaluation:{ENDC}\")\n","for i, dataset_name in enumerate(datasets.keys()):\n","    print(f\"{i+1}. {dataset_name}\")"]},{"cell_type":"markdown","metadata":{"id":"VldzRMl-yHwm"},"source":["Test a single sample for all the datasets to ensure everything is working first"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DKA7grocyHwn","colab":{"base_uri":"https://localhost:8080/","height":723},"executionInfo":{"status":"error","timestamp":1741501749222,"user_tz":480,"elapsed":4410,"user":{"displayName":"Jeremy Huang","userId":"00765697131879146443"}},"outputId":"bb797da1-b1eb-44bd-d8ab-ae68a593cac1"},"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[96mLoading VitaminC dataset from /content/drive/Shareddrives/517 nlp project/data/VitaminC/vitaminc/test.jsonl...\u001b[0m\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":["\u001b[96mLoaded 1 samples from VitaminC.\u001b[0m\n","\u001b[1m\u001b[96mStarting evaluation of VitaminC dataset...\u001b[0m\n","\n","\u001b[1m================================================================================\u001b[0m\n","\u001b[94m\u001b[1mSAMPLE 1/1 - DATASET: VitaminC\u001b[0m\n","\u001b[94m--------------------------------------------------------------------------------\u001b[0m\n","\u001b[92mPROMPT:\u001b[0m\n","\u001b[92mClaim: Westlife made under 23.5 million sales in the UK .\n","\n","Evidence: According to the British Phonographic Industry ( BPI ) , Westlife has been certified for 13 million albums and 9.8�million singles , with a total of more than 23 million combined sales in the UK .\n","\n","Think step-by-step to determine if the evidence SUPPORTS, REFUTES, or provides NOT ENOUGH INFO for the claim.\n","\n","After your thinking, end your answer with one of these in a latex boxed format:\n","$\\boxed{SUPPORTS}$ or $\\boxed{REFUTES}$ or $\\boxed{NOT ENOUGH INFO}$\n","\n","Always end with your answer in a latex boxed format:\n","$\\boxed{<finalAnswer>}$\u001b[0m\n","\u001b[94m--------------------------------------------------------------------------------\u001b[0m\n"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-12-31cfd0e6ac17>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0;31m# Evaluate on the dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m         \u001b[0maccuracy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m         \u001b[0msummary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdataset_name\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maccuracy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-9-f8973bdc6614>\u001b[0m in \u001b[0;36mevaluate_dataset\u001b[0;34m(model, tokenizer, data, dataset_name)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0;31m# Run inference\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_inference\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprompt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0;31m# Print the model's response\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-5-fa15cb3b9cb2>\u001b[0m in \u001b[0;36mrun_inference\u001b[0;34m(model, tokenizer, prompt, device)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         outputs = model.generate(\n\u001b[0m\u001b[1;32m     13\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'input_ids'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'attention_mask'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/_contextlib.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    114\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mctx_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/generation/utils.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   2253\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2254\u001b[0m             \u001b[0;31m# 12. run sample (it degenerates to greedy search when `generation_config.do_sample=False`)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2255\u001b[0;31m             result = self._sample(\n\u001b[0m\u001b[1;32m   2256\u001b[0m                 \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2257\u001b[0m                 \u001b[0mlogits_processor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprepared_logits_processor\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/generation/utils.py\u001b[0m in \u001b[0;36m_sample\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[1;32m   3241\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3242\u001b[0m         \u001b[0mis_prefill\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3243\u001b[0;31m         while self._has_unfinished_sequences(\n\u001b[0m\u001b[1;32m   3244\u001b[0m             \u001b[0mthis_peer_finished\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msynced_gpus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcur_len\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcur_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_length\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3245\u001b[0m         ):\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["# Choose which datasets to evaluate\n","datasets_to_evaluate = [\n","    \"VitaminC\",\n","    \"FEVER\",\n","    \"FEVEROUS\",\n","    \"HotpotQA\",\n","    \"2WikiMultihopQA\",\n","    \"SVAMP\",\n","    \"Bamboogle\"\n","]  # Evaluate all available datasets\n","\n","# Initialize results summary\n","summary = {}\n","\n","# Process each selected dataset\n","for dataset_name in datasets_to_evaluate:\n","    if dataset_name not in datasets:\n","        print(f\"{RED}Dataset {dataset_name} not found in available datasets{ENDC}\")\n","        continue\n","\n","    # Load dataset\n","    file_path = datasets[dataset_name]\n","    data = load_dataset(file_path, dataset_name, limit=1)  # Limit to 1 sample\n","\n","    if data:\n","        # Evaluate on the dataset\n","        accuracy, _ = evaluate_dataset(model, tokenizer, data, dataset_name)\n","        summary[dataset_name] = accuracy"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Bakui531yHwn","colab":{"base_uri":"https://localhost:8080/","output_embedded_package_id":"1gVwwjJT-tVl-GwLgj7do7zFFWsVb8VWK"},"outputId":"cc54408a-9a54-4a7e-9f4a-63e954b311d7"},"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}],"source":["# Choose which datasets to evaluate\n","# You can modify this list to evaluate specific datasets\n","# datasets_to_evaluate = [\"VitaminC\", \"FEVER\"]  # Uncomment to evaluate specific datasets\n","datasets_to_evaluate = [\n","    \"VitaminC\",\n","    # \"FEVER\",\n","    # \"FEVEROUS\",\n","    \"HotpotQA\",\n","    \"2WikiMultihopQA\",\n","    \"SVAMP\",\n","    \"Bamboogle\"\n","]  # Evaluate all available datasets\n","# Initialize results summary\n","summary = {}\n","\n","# Process each selected dataset\n","for dataset_name in datasets_to_evaluate:\n","    if dataset_name not in datasets:\n","        print(f\"{RED}Dataset {dataset_name} not found in available datasets{ENDC}\")\n","        continue\n","\n","    # Load dataset\n","    file_path = datasets[dataset_name]\n","    data = load_dataset(file_path, dataset_name, limit=150)  # Limit to 150 samples\n","\n","    if data:\n","        # Evaluate on the dataset\n","        accuracy, _ = evaluate_dataset(model, tokenizer, data, dataset_name)\n","        summary[dataset_name] = accuracy"]},{"cell_type":"markdown","metadata":{"id":"QPgP1zLsyHwn"},"source":["## Results Visualization\n","\n","Create summary visualizations of model performance across datasets"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BOOg_Y7OyHwn","colab":{"base_uri":"https://localhost:8080/","height":210},"executionInfo":{"status":"error","timestamp":1741508386396,"user_tz":480,"elapsed":108,"user":{"displayName":"Jeremy Huang","userId":"00765697131879146443"}},"outputId":"44fb35e5-1a59-4e64-d744-498d33fa3aea"},"outputs":[{"output_type":"error","ename":"NameError","evalue":"name 'BOLD' is not defined","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-1-edcace2a12d1>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Print summary of results\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"\\n{BOLD}{CYAN}Summary of Results:{ENDC}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccuracy\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msummary\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{dataset}: {accuracy:.4f}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'BOLD' is not defined"]}],"source":["# Print summary of results\n","print(f\"\\n{BOLD}{CYAN}Summary of Results:{ENDC}\")\n","for dataset, accuracy in summary.items():\n","    print(f\"{dataset}: {accuracy:.4f}\")\n","\n","# Create summary visualization\n","if summary:\n","    plt.figure(figsize=(12, 6))\n","    datasets = list(summary.keys())\n","    accuracies = list(summary.values())\n","\n","    # Sort by accuracy\n","    sorted_indices = np.argsort(accuracies)[::-1]\n","    sorted_datasets = [datasets[i] for i in sorted_indices]\n","    sorted_accuracies = [accuracies[i] for i in sorted_indices]\n","\n","    bars = plt.bar(sorted_datasets, sorted_accuracies)\n","\n","    # Add value labels on top of bars\n","    for bar in bars:\n","        height = bar.get_height()\n","        plt.text(bar.get_x() + bar.get_width()/2., height + 0.01,\n","                f'{height:.4f}', ha='center', va='bottom')\n","\n","    plt.ylim(0, 1.1)  # Set y limit to 0-1 with a small margin\n","    plt.xlabel('Datasets')\n","    plt.ylabel('Accuracy')\n","    plt.title('Llama-3.2-1B Performance Across Datasets')\n","    plt.xticks(rotation=45)\n","    plt.tight_layout()\n","    plt.savefig(\"summary_results.png\")\n","    plt.show()\n","\n","    # Save summary to CSV\n","    summary_df = pd.DataFrame(list(summary.items()), columns=['Dataset', 'Accuracy'])\n","    summary_df = summary_df.sort_values('Accuracy', ascending=False)\n","    summary_df.to_csv(\"summary_results.csv\", index=False)\n","\n","    print(f\"\\n{GREEN}Summary saved to summary_results.csv and summary_results.png{ENDC}\")\n","else:\n","    print(f\"\\n{RED}No results to visualize{ENDC}\")"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
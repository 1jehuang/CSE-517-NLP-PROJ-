{"cells":[{"cell_type":"markdown","metadata":{"id":"lIhkyC3Vwtzd"},"source":["# Truncate Training Datasets\n","\n","This notebook loads the original training datasets and creates truncated versions with only the first 300 samples from each dataset."]},{"cell_type":"code","execution_count":14,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PTwzEA7Lwtze","executionInfo":{"status":"ok","timestamp":1741589065405,"user_tz":420,"elapsed":1158,"user":{"displayName":"Jeremy Huang","userId":"12545693065398382142"}},"outputId":"212c72ec-5a6c-4d21-e80f-9b3365738727"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["import json\n","import os\n","from google.colab import drive\n","\n","# Mount Google Drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":15,"metadata":{"id":"BOdFLTyOwtze","executionInfo":{"status":"ok","timestamp":1741589065414,"user_tz":420,"elapsed":14,"user":{"displayName":"Jeremy Huang","userId":"12545693065398382142"}}},"outputs":[],"source":["# Define the paths to the training data\n","train_paths = [\n","    '/content/drive/Shareddrives/517 nlp project/data/2WikiMultihopQA/train.json',\n","    '/content/drive/Shareddrives/517 nlp project/data/Bamboogle/train.json',\n","    '/content/drive/Shareddrives/517 nlp project/data/FEVER/fever_train.jsonl',\n","    '/content/drive/Shareddrives/517 nlp project/data/FEVEROUS/feverous_train.jsonl',\n","    '/content/drive/Shareddrives/517 nlp project/data/HotpotQA/train.json',\n","    '/content/drive/Shareddrives/517 nlp project/data/SVAMP/train.json',\n","    '/content/drive/Shareddrives/517 nlp project/data/VitaminC/vitaminc/train.jsonl'\n","]\n","\n","# Number of samples to keep\n","num_samples = 300"]},{"cell_type":"code","execution_count":16,"metadata":{"id":"_C2XVxm3wtzf","executionInfo":{"status":"ok","timestamp":1741589065419,"user_tz":420,"elapsed":17,"user":{"displayName":"Jeremy Huang","userId":"12545693065398382142"}}},"outputs":[],"source":["# Function to check if a file is in JSONL format\n","def is_jsonl(file_path):\n","    # Check if file extension is jsonl\n","    if file_path.lower().endswith('.jsonl'):\n","        return True\n","\n","    # If not explicit in extension, peek at first few lines\n","    try:\n","        with open(file_path, 'r', encoding='utf-8') as f:\n","            first_lines = [f.readline().strip() for _ in range(3)]\n","\n","        # Check if each non-empty line is a valid JSON object\n","        valid_lines = 0\n","        for line in first_lines:\n","            if not line:\n","                continue\n","            try:\n","                json.loads(line)\n","                valid_lines += 1\n","            except json.JSONDecodeError:\n","                return False\n","\n","        # If we found multiple valid JSON lines, likely JSONL format\n","        return valid_lines > 1\n","    except Exception:\n","        return False\n","\n","# Function to truncate JSON dataset and save to new file\n","def truncate_dataset(file_path, num_samples):\n","    try:\n","        # Get the directory and filename\n","        directory = os.path.dirname(file_path)\n","        filename = os.path.basename(file_path)\n","        dataset_name = os.path.basename(os.path.dirname(file_path))\n","\n","        # Create output filename with _truncated suffix\n","        base_filename = os.path.splitext(filename)[0]\n","        extension = os.path.splitext(filename)[1]\n","        output_filename = base_filename + f\"_truncated_{num_samples}\" + extension\n","        output_path = os.path.join(directory, output_filename)\n","\n","        print(f\"Processing {dataset_name}...\")\n","\n","        # Check if the file is in JSONL format\n","        if is_jsonl(file_path):\n","            print(f\"  Detected JSONL format for {dataset_name}\")\n","            # Process JSONL file line by line\n","            with open(file_path, 'r', encoding='utf-8') as f_in:\n","                lines = [line.strip() for line in f_in if line.strip()]\n","\n","            # Count original samples\n","            original_count = len(lines)\n","            # Truncate to the specified number of samples\n","            truncated_lines = lines[:num_samples]\n","\n","            # Write truncated lines to new file\n","            with open(output_path, 'w', encoding='utf-8') as f_out:\n","                for line in truncated_lines:\n","                    f_out.write(line + '\\n')\n","\n","            print(f\"  Original samples: {original_count}, Truncated to: {len(truncated_lines)}\")\n","        else:\n","            # Process as regular JSON\n","            with open(file_path, 'r', encoding='utf-8') as f:\n","                data = json.load(f)\n","\n","            # Determine the structure of the data and truncate accordingly\n","            if isinstance(data, list):\n","                # If data is a list, simply take the first num_samples items\n","                truncated_data = data[:num_samples]\n","                print(f\"  Original samples: {len(data)}, Truncated to: {len(truncated_data)}\")\n","            elif isinstance(data, dict):\n","                # If data is a dictionary, we need to figure out which key contains the data\n","                # Try common keys like 'data', 'examples', etc.\n","                data_keys = [k for k in data.keys() if isinstance(data[k], list)]\n","\n","                if data_keys:\n","                    main_key = data_keys[0]  # Use the first list as the main data\n","                    truncated_data = dict(data)\n","                    truncated_data[main_key] = data[main_key][:num_samples]\n","                    print(f\"  Original samples: {len(data[main_key])}, Truncated to: {len(truncated_data[main_key])}\")\n","                else:\n","                    # If no lists found, just use the original data (though this is unexpected)\n","                    truncated_data = data\n","                    print(f\"  Warning: Could not identify data structure to truncate in {dataset_name}\")\n","            else:\n","                print(f\"  Error: Unexpected data format in {dataset_name}\")\n","                return None\n","\n","            # Save the truncated data\n","            with open(output_path, 'w', encoding='utf-8') as f:\n","                json.dump(truncated_data, f, ensure_ascii=False, indent=2)\n","\n","        print(f\"  Saved to {output_path}\")\n","        return output_path\n","\n","    except Exception as e:\n","        print(f\"  Error processing {file_path}: {str(e)}\")\n","        return None"]},{"cell_type":"code","execution_count":17,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"g4YV3Njuwtzf","executionInfo":{"status":"ok","timestamp":1741589084200,"user_tz":420,"elapsed":18780,"user":{"displayName":"Jeremy Huang","userId":"12545693065398382142"}},"outputId":"d6a017ee-f85f-4f2a-fb0c-03c61d60a2d0"},"outputs":[{"output_type":"stream","name":"stdout","text":["Processing 2WikiMultihopQA...\n","  Original samples: 167454, Truncated to: 300\n","  Saved to /content/drive/Shareddrives/517 nlp project/data/2WikiMultihopQA/train_truncated_300.json\n","Processing Bamboogle...\n","  Original samples: 125, Truncated to: 125\n","  Saved to /content/drive/Shareddrives/517 nlp project/data/Bamboogle/train_truncated_300.json\n","Processing FEVER...\n","  Detected JSONL format for FEVER\n","  Original samples: 116359, Truncated to: 300\n","  Saved to /content/drive/Shareddrives/517 nlp project/data/FEVER/fever_train_truncated_300.jsonl\n","Processing FEVEROUS...\n","  Detected JSONL format for FEVEROUS\n","  Original samples: 57033, Truncated to: 300\n","  Saved to /content/drive/Shareddrives/517 nlp project/data/FEVEROUS/feverous_train_truncated_300.jsonl\n","Processing HotpotQA...\n","  Original samples: 90447, Truncated to: 300\n","  Saved to /content/drive/Shareddrives/517 nlp project/data/HotpotQA/train_truncated_300.json\n","Processing SVAMP...\n","  Original samples: 700, Truncated to: 300\n","  Saved to /content/drive/Shareddrives/517 nlp project/data/SVAMP/train_truncated_300.json\n","Processing vitaminc...\n","  Detected JSONL format for vitaminc\n","  Original samples: 370653, Truncated to: 300\n","  Saved to /content/drive/Shareddrives/517 nlp project/data/VitaminC/vitaminc/train_truncated_300.jsonl\n","\n","Summary:\n","Successfully truncated 7 out of 7 datasets\n"]}],"source":["# Process each dataset\n","truncated_files = []\n","\n","for path in train_paths:\n","    output_path = truncate_dataset(path, num_samples)\n","    if output_path:\n","        truncated_files.append(output_path)\n","\n","print(\"\\nSummary:\")\n","print(f\"Successfully truncated {len(truncated_files)} out of {len(train_paths)} datasets\")"]},{"cell_type":"code","execution_count":18,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BAfI960awtzg","executionInfo":{"status":"ok","timestamp":1741589084316,"user_tz":420,"elapsed":115,"user":{"displayName":"Jeremy Huang","userId":"12545693065398382142"}},"outputId":"a4a2af5a-9ff2-44be-bf76-00120f075f98"},"outputs":[{"output_type":"stream","name":"stdout","text":["2WikiMultihopQA: 300 samples\n","Bamboogle: 125 samples\n","FEVER: 300 samples (JSONL format)\n","FEVEROUS: 300 samples (JSONL format)\n","HotpotQA: 300 samples\n","SVAMP: 300 samples\n","vitaminc: 300 samples (JSONL format)\n"]}],"source":["# Verify the content of truncated files\n","for file_path in truncated_files:\n","    dataset_name = os.path.basename(os.path.dirname(file_path))\n","\n","    # Check if the file is in JSONL format\n","    if is_jsonl(file_path):\n","        # Count the number of lines in the file\n","        with open(file_path, 'r', encoding='utf-8') as f:\n","            line_count = sum(1 for line in f if line.strip())\n","        print(f\"{dataset_name}: {line_count} samples (JSONL format)\")\n","    else:\n","        # Regular JSON format\n","        with open(file_path, 'r', encoding='utf-8') as f:\n","            data = json.load(f)\n","\n","        if isinstance(data, list):\n","            print(f\"{dataset_name}: {len(data)} samples\")\n","        elif isinstance(data, dict):\n","            # Find the first list in the dictionary\n","            for key, value in data.items():\n","                if isinstance(value, list):\n","                    print(f\"{dataset_name}: {len(value)} samples in '{key}'\")\n","                    break\n","            else:\n","                print(f\"{dataset_name}: Structure unclear\")"]}],"metadata":{"language_info":{"name":"python"},"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"nbformat":4,"nbformat_minor":0}
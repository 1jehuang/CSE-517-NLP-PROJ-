{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"ELRN4OHpujxD"},"outputs":[],"source":["import json\n","import time\n","import re\n","import random\n","import os\n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","# from google.colab import drive\n","from transformers import AutoModelForCausalLM, AutoTokenizer\n","from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n","import torch\n","import math\n","from transformers import pipeline\n","\n","\n","def load_dataset(file_path, dataset_name, limit=150):\n","    \"\"\"Load samples from various datasets with appropriate format handling\"\"\"\n","    print(f\"Loading {dataset_name} dataset from {file_path}...\")\n","    data = []\n","\n","    if not os.path.exists(file_path):\n","        print(f\"File not found: {file_path}\")\n","        return []\n","\n","    if file_path.endswith('.jsonl'):\n","        with open(file_path, 'r', encoding='utf-8') as f:\n","            for line in f:\n","                if line.strip():  # Skip empty lines\n","                    data.append(json.loads(line))\n","                    if len(data) >= limit:\n","                        break\n","    else:  # .json files\n","        with open(file_path, 'r', encoding='utf-8') as f:\n","            json_data = json.load(f)\n","            if isinstance(json_data, list):\n","                data = json_data[:limit]\n","            else:\n","                # Handle nested structures if needed\n","                if 'data' in json_data:\n","                    data = json_data['data'][:limit]\n","                else:\n","                    print(f\"Warning: Unexpected JSON structure in {file_path}\")\n","                    data = [json_data]  # Just use the whole object as one sample\n","\n","    print(f\"Loaded {len(data)} samples from {dataset_name}.\")\n","    return data\n","\n","def create_prompt(sample, dataset_name):\n","    \"\"\"Create appropriate prompts based on dataset type\"\"\"\n","\n","    if dataset_name == \"VitaminC\" or dataset_name == \"FEVER\" or dataset_name == \"FEVEROUS\":\n","        # Fact verification datasets\n","        if dataset_name == \"VitaminC\":\n","            claim = sample[\"claim\"]\n","            evidence = sample[\"evidence\"]\n","        elif dataset_name == \"FEVER\":\n","            claim = sample[\"claim\"]\n","            evidence = sample.get(\"evidence\", \"\") or sample.get(\"context\", \"\")\n","        elif dataset_name == \"FEVEROUS\":\n","            claim = sample[\"claim\"]\n","            evidence = sample.get(\"evidence\", \"\") or sample.get(\"context\", \"\")\n","\n","        return create_fact_verification_prompt(claim, evidence)\n","\n","    elif dataset_name == \"HotpotQA\" or dataset_name == \"2WikiMultihopQA\":\n","        # Multi-hop QA datasets\n","        question = sample.get(\"question\", \"\") or sample.get(\"query\", \"\")\n","        context = sample.get(\"context\", \"\")\n","        if not context and \"original_context\" in sample:\n","            context = sample[\"original_context\"]\n","\n","        return create_qa_prompt(question, context)\n","\n","    elif dataset_name == \"SVAMP\":\n","        # Math word problem dataset\n","        question = sample.get(\"question\", \"\") or sample.get(\"body\", \"\")\n","        return create_math_prompt(question)\n","\n","    elif dataset_name == \"Bamboogle\":\n","        # Bamboogle dataset - assume it's a QA task\n","        question = sample.get(\"question\", \"\")\n","        context = sample.get(\"context\", \"\")\n","\n","        if not question and \"answer\" in sample:\n","            # If no question is provided but there's an answer, create a generic prompt\n","            return\n","\n","        return create_qa_prompt(question, context)\n","\n","    else:\n","        # Generic prompt for unknown datasets\n","        return f\"Please analyze this data and think step-by-step:\\n\\n{json.dumps(sample, indent=2)}\\n\\nAfter your thinking, provide your answer by writing 'ANSWER: <finalAnswer>' on the same line.\"\n","\n","def create_fact_verification_prompt(claim, evidence):\n","    \"\"\"Create a prompt for fact checking\"\"\"\n","    prompt = f\"\"\"Claim: {claim}\n","\n","Evidence: {evidence}\n","\n","Think step-by-step to determine if the evidence SUPPORTS, REFUTES, or provides NOT ENOUGH INFO for the claim.\n","\n","After your thinking, end your answer with one of these:\n","ANSWER: SUPPORTS or ANSWER: REFUTES or ANSWER: NOT ENOUGH INFO\n","\n","Always end with the final answer.\"\"\"\n","    return prompt\n","\n","def create_qa_prompt(question, context):\n","    \"\"\"Create a prompt for question answering tasks\"\"\"\n","    prompt = f\"\"\"\n","\n","Question: {question}\n","\n","Think step-by-step to answer the question based on the context.\n","\n","After your thinking, provide your answer in this exact format:\n","ANSWER: <finalAnswer>\n","\n","Always end with the final answer.\"\"\"\n","    return prompt\n","\n","def create_math_prompt(question):\n","    \"\"\"Create a prompt for math word problems\"\"\"\n","    prompt = f\"\"\"Problem: {question}\n","\n","Think step-by-step to solve this math problem.\n","\n","After your thinking, provide your final numeric answer in this exact format:\n","ANSWER: <finalAnswer>\n","Never end your final answer with a latex boxed format.\n","Always end with the final answer.\"\"\"\n","    return prompt\n","\n","\n","def run_inference(pipe, prompt, device=\"cuda\"):\n","    \"\"\"Run inference on the model\"\"\"\n","    messages = [\n","        {\"role\": \"system\", \"content\": \"You are a helpful assistant\"},\n","        {\"role\": \"user\", \"content\": prompt}\n","    ]\n","    # add chat template\n","    # messages = pipe.tokenizer.apply_chat_template(messages, tokenize=False)\n","    outputs = pipe(\n","        messages,\n","        max_new_tokens=2048,\n","        temperature=0.2,\n","        pad_token_id=pipe.tokenizer.eos_token_id\n","    )\n","    # print(outputs)\n","    return outputs[0][\"generated_text\"][-1]['content']\n","\n","    # # Tokenize input with attention mask\n","    # encoding = tokenizer(prompt, return_tensors=\"pt\", padding=True, truncation=True, max_length=128000)\n","\n","    # inputs = {\n","    #     'input_ids': encoding.input_ids.to(device),\n","    #     'attention_mask': encoding.attention_mask.to(device)\n","    # }\n","\n","    # with torch.no_grad():\n","    #     outputs = model.generate(\n","    #         input_ids=inputs['input_ids'],\n","    #         attention_mask=inputs['attention_mask'],\n","    #         max_new_tokens=2048,  # Adjust based on desired output length\n","    #         temperature=0.2,      # Lower temperature for focused output\n","    #         do_sample=True,\n","    #     )\n","\n","    # return tokenizer.decode(outputs[0][encoding.shape[1]:], skip_special_tokens=True)\n","\n","import re\n","\n","def extract_prediction(output, dataset_name, sample=None):\n","    \"\"\"Extract the final answer from the model's output.\n","    1) Tries to find lines starting with 'ANSWER:' or 'The final answer is:' and extracts the last one.\n","    2) If none found, falls back to dataset-specific extraction.\n","    \"\"\"\n","    pattern = r'(?:^|\\n)(?:ANSWER:|The final answer is:)\\s*(.*)$'\n","    matches = re.findall(pattern, output, flags=re.MULTILINE)\n","\n","    if matches:\n","        last_answer = matches[-1].strip()\n","        if dataset_name == \"SVAMP\":\n","            parsed = _handle_math_answer(last_answer, output)\n","            return parsed\n","        return last_answer\n","\n","    # If no direct lines found, fall back to dataset-specific extraction.\n","    extraction_methods = {\n","        \"VitaminC\": extract_fact_verification_prediction,\n","        \"FEVER\": extract_fact_verification_prediction,\n","        \"FEVEROUS\": extract_fact_verification_prediction,\n","        \"HotpotQA\": extract_qa_prediction,\n","        \"2WikiMultihopQA\": extract_qa_prediction,\n","        \"SVAMP\": extract_math_prediction,  # fallback if we can't find any line above\n","        \"Bamboogle\": extract_qa_prediction,\n","    }\n","\n","    # Attempt dataset-specific extraction.\n","    return extraction_methods.get(dataset_name, lambda x: x.strip())(output)\n","\n","\n","def extract_fact_verification_prediction(output):\n","    \"\"\"Extract fact verification label from model output.\"\"\"\n","    output_lower = output.lower()\n","\n","    # 1) Check if there's an ANSWER: line with a direct label.\n","    answers = re.findall(r'ANSWER:\\s*(SUPPORTS|REFUTES|NOT ENOUGH INFO)', output_lower, re.MULTILINE)\n","    if answers:\n","        return answers[-1]\n","\n","    # 2) Look for known patterns in the text.\n","    verdict_patterns = [\n","        r\"verdict\\s*:?\\s*(supports|refutes|not enough info)\",\n","        r\"therefore,?\\s+(the evidence)?\\s*(supports|refutes|provides not enough info)\",\n","        r\"(my conclusion|my answer) is\\s*:?\\s*(supports|refutes|not enough info)\",\n","        r\"the evidence (supports|refutes|provides not enough info)\",\n","        r\"(supports|refutes|not enough info) the claim\"\n","    ]\n","    for pattern in verdict_patterns:\n","        matches = re.findall(pattern, output_lower)\n","        if matches:\n","            label = matches[-1][-1] if isinstance(matches[-1], tuple) else matches[-1]\n","            return _normalize_fact_label(label)\n","\n","    # 3) Try searching the last quarter or entire text.\n","    last_quarter = output_lower[3*len(output_lower)//4:]\n","    return _search_label(last_quarter, output_lower)\n","\n","\n","def extract_qa_prediction(output):\n","    \"\"\"Extract the answer from QA model output.\"\"\"\n","    answers = re.findall(r'^ANSWER:\\s*(.*?)$', output, re.MULTILINE)\n","    if answers:\n","        return answers[-1].strip()\n","    return _extract_last_non_empty_line(output)\n","\n","\n","def extract_math_prediction(output):\n","    \"\"\"Extract a numeric or fraction answer for math problems, including fallback.\"\"\"\n","    answers = re.findall(r'^ANSWER:\\s*(.*)$', output, re.MULTILINE)\n","    if answers:\n","        raw_answer = answers[-1].strip()\n","        parsed = _handle_math_answer(raw_answer, output)\n","        return parsed\n","    return _extract_last_number(output)\n","\n","\n","def _handle_math_answer(last_answer, full_output):\n","    \"\"\"Parse fraction/boxed/numeric answers from the last answer string.\n","    If no fraction or numeric is found, fallback to the last number in the entire output.\n","    \"\"\"\n","    # A) Check if there's a \\boxed{...}.\n","    box_match = re.findall(r'\\\\?boxed\\s*\\{([^}]*)\\}', last_answer)\n","    if box_match:\n","        return _handle_math_answer(box_match[-1].strip(), full_output)\n","\n","    # B) Check for latex fraction: \\frac{num}{den}.\n","    fraction_latex = re.findall(r'\\\\?frac\\{\\s*(-?[\\d\\.]+)\\s*\\}\\{\\s*([\\d\\.]+)\\s*\\}', last_answer)\n","    if fraction_latex:\n","        num, den = fraction_latex[-1]\n","        return f\"{num}/{den}\"\n","\n","    # C) Plain fraction: e.g. 20/7.\n","    fraction_plain = re.findall(r'(-?[\\d\\.]+\\s*/\\s*[\\d\\.]+)', last_answer)\n","    if fraction_plain:\n","        return fraction_plain[-1].replace(' ', '')\n","\n","    # D) Numeric.\n","    numeric_matches = re.findall(r'(-?[\\d,]+\\.?\\d*)', last_answer)\n","    if numeric_matches:\n","        numeric_value = _convert_to_float(numeric_matches[-1])\n","        return numeric_value\n","\n","    # E) If no fraction/numeric found, fallback to the last numeric in the entire model output.\n","    fallback_num = _extract_last_number(full_output)\n","    return fallback_num\n","\n","\n","def _convert_to_float(value):\n","    \"\"\"Convert string to float if possible, else return original.\"\"\"\n","    try:\n","        clean_value = value.replace(',', '')\n","        clean_value = re.sub(r'[\\\\$\\{\\}]', '', clean_value)\n","        return float(clean_value)\n","    except ValueError:\n","        return value\n","\n","\n","def _normalize_fact_label(label):\n","    label_lower = label.lower()\n","    if \"not enough\" in label_lower:\n","        return \"NOT ENOUGH INFO\"\n","    if \"supports\" in label_lower:\n","        return \"SUPPORTS\"\n","    if \"refutes\" in label_lower:\n","        return \"REFUTES\"\n","    return \"NOT ENOUGH INFO\"\n","\n","\n","def _search_label(last_quarter, output):\n","    for label in [\"not enough info\", \"supports\", \"refutes\"]:\n","        if label in last_quarter or label in output:\n","            return _normalize_fact_label(label)\n","    return \"NOT ENOUGH INFO\"\n","\n","\n","def _extract_last_non_empty_line(output):\n","    lines = [line.strip() for line in output.split('\\n') if line.strip()]\n","    return lines[-1] if lines else output.strip()\n","\n","\n","def _extract_last_number(output):\n","    lines = output.split('\\n')[-5:]\n","    for line in reversed(lines):\n","        numbers = re.findall(r\"(-?[\\d,]+\\.?\\d*)\", line)\n","        for num in reversed(numbers):\n","            converted = _convert_to_float(num)\n","            if isinstance(converted, float):\n","                return converted\n","    return output.strip()\n","\n","\n","def evaluate_correctness(prediction, ground_truth, dataset_name):\n","    \"\"\"Evaluate if the prediction is correct based on dataset type\"\"\"\n","\n","    if dataset_name == \"VitaminC\" or dataset_name == \"FEVER\" or dataset_name == \"FEVEROUS\":\n","        # Fact verification - direct comparison\n","        return prediction == ground_truth\n","\n","    elif dataset_name == \"HotpotQA\" or dataset_name == \"2WikiMultihopQA\" or dataset_name == \"Bamboogle\":\n","        # QA evaluation - normalize and compare\n","        return normalize_qa_answers(prediction, ground_truth)\n","\n","    elif dataset_name == \"SVAMP\":\n","        # Math evaluation - numeric comparison\n","        return evaluate_math_correctness(prediction, ground_truth)\n","\n","    else:\n","        # Generic comparison for unknown datasets\n","        return prediction == ground_truth\n","\n","def normalize_qa_answers(prediction, ground_truth):\n","    \"\"\"Normalize and compare QA answers with flexible matching\"\"\"\n","    if not prediction or not ground_truth:\n","        return False\n","\n","    # Handle list or dictionary ground truths\n","    if isinstance(ground_truth, list):\n","        ground_truth = \" \".join([str(item) for item in ground_truth])\n","    elif isinstance(ground_truth, dict):\n","        if \"answer\" in ground_truth:\n","            ground_truth = ground_truth[\"answer\"]\n","        else:\n","            ground_truth = str(ground_truth)\n","\n","    # Normalize both strings\n","    pred_norm = prediction.lower().strip()\n","    truth_norm = str(ground_truth).lower().strip()\n","\n","    # Remove punctuation and extra spaces\n","    pred_norm = re.sub(r'[^\\w\\s]', '', pred_norm).strip()\n","    truth_norm = re.sub(r'[^\\w\\s]', '', truth_norm).strip()\n","\n","    # Check if prediction contains ground truth or vice versa\n","    return pred_norm in truth_norm or truth_norm in pred_norm\n","\n","def evaluate_math_correctness(prediction, ground_truth):\n","    \"\"\"Evaluate correctness of math answers with tolerance\"\"\"\n","    try:\n","        # Convert to numeric values\n","        if isinstance(prediction, str):\n","            prediction = float(re.search(r'(-?[\\d.]+)', prediction.replace(',', '')).group(1))\n","\n","        if isinstance(ground_truth, str):\n","            ground_truth = float(re.search(r'(-?[\\d.]+)', ground_truth.replace(',', '')).group(1))\n","\n","        # Compare with tolerance\n","        tolerance = 0.01\n","        return abs(float(prediction) - float(ground_truth)) < tolerance\n","    except (ValueError, TypeError, AttributeError):\n","        return False\n","\n","def get_ground_truth(sample, dataset_name):\n","    \"\"\"Extract ground truth from sample based on dataset type\"\"\"\n","\n","    if dataset_name == \"VitaminC\":\n","        return sample.get(\"label\", \"\")\n","\n","    elif dataset_name == \"FEVER\" or dataset_name == \"FEVEROUS\":\n","        return sample.get(\"label\", \"\")\n","\n","    elif dataset_name == \"HotpotQA\" or dataset_name == \"2WikiMultihopQA\":\n","        return sample.get(\"answer\", \"\")\n","\n","    elif dataset_name == \"SVAMP\":\n","        return sample.get(\"answer\", None)\n","\n","    elif dataset_name == \"Bamboogle\":\n","        return sample.get(\"answer\", \"\")\n","\n","    else:\n","        return None\n","\n","\n","def generate_reasoning_evaluate_prompt(question, y):\n","    prompt = f\"\"\"Evaluate if the given reasoning can lead to the right solution to the question. Choose one word from (correct, likely, impossible) to indicate your evaluation of the reasoning's quality. Do not output anything else.\n","Question: {question}\n","Reasoning: {y}\n","Evaluation (choose from correct/likely/impossible): \"\"\"\n","    return prompt\n","\n","def get_new_ys(x, ys, step, n_generate_sample, pipe):\n","    '''\n","    x is the question prompt\n","    ys is the current output candidates (from step 1 to step {step-1})\n","    step is the current step\n","    n_generate_sample is the number of new output candidates to generate\n","    return the new output candidates (from step 1 to step {step})\n","    '''\n","    new_ys = []\n","    for y in ys:\n","        for _ in range(n_generate_sample):\n","            prompt = x + y + \"Step \" + str(step) + \", \"\n","            # print(prompt)\n","            new_y = run_inference(pipe, prompt)\n","            # clean new_y and only get the thought of this step\n","            ## cut if there's Step {step+1} or The answer is\n","            new_y = new_y.split(\"Step \" + str(step+1))[0]\n","            # new_y = new_y.split(\"The answer is \")[0]\n","            new_ys.append(y + \"Step \" + str(step) + \", \" + new_y)\n","    return new_ys\n","\n","\n","def get_new_ys_last_step(x, ys, n_generate_sample, pipe):\n","    '''\n","    x is the question prompt\n","    ys is the current output candidates (from step 1 to step {step-1})\n","    step is the current step\n","    n_generate_sample is the number of new output candidates to generate\n","    return the new output candidates (from step 1 to step {step})\n","    '''\n","    new_ys = []\n","    for y in ys:\n","        for _ in range(n_generate_sample):\n","            prompt = x + y + \"ANSWER: \"\n","            # print(prompt)\n","            new_y = run_inference(pipe, prompt)\n","            # clean new_y and only get the thought of this step\n","            ## cut if there's Step {step+1} or The answer is\n","\n","            new_ys.append(y + \"ANSWER: \" + new_y)\n","    return new_ys\n","\n","\n","def evaluate_state(pipe, x, y):\n","    prompt = generate_reasoning_evaluate_prompt(x, y)\n","    response = run_inference(pipe, prompt).lower()\n","    print(response)\n","    if 'impossible' in response or 'incorrect' in response or 'unlikely' or 'never' in response:\n","        return 0\n","    elif 'likely' in response:\n","        return 0.5\n","    elif 'correct' in response:\n","        return 1\n","    else:\n","        return 0.01\n","\n","def solve(pipe, prompt, n_generate_sample=6, n_select_sample=3, depth=3, verbose=False):\n","    x = prompt\n","    ys = [\"\"]\n","    infos = []\n","    for step in range(1, depth + 1):\n","        if step < depth:\n","            new_ys = get_new_ys(x, ys, step, n_generate_sample, pipe)\n","        else:\n","            new_ys = get_new_ys_last_step(x, ys, n_generate_sample, pipe)\n","        if verbose:\n","            print(f\"Step {step}: {new_ys}\")\n","        ids = list(range(len(new_ys)))\n","\n","        # evaluation\n","        values = [evaluate_state(pipe, x, y) for y in new_ys]\n","\n","        # select top n_select_sample\n","        ps = np.array(values) / np.sum(values)\n","        ## if the number of impossible is more than n_generate_sample - n_select_sample, we sample with replacement\n","        if step == depth:\n","            n_select_sample = 1\n","        if np.sum(np.array(values) == 0) > n_generate_sample - n_select_sample:\n","            values = np.array(values) + 1e-6\n","            ps = values / np.sum(values)\n","            selected_ids = np.random.choice(ids, size=n_select_sample, p=ps, replace=True)\n","        else:\n","            selected_ids = np.random.choice(ids, size=n_select_sample, p=ps, replace=False)\n","        select_new_ys = [new_ys[i] for i in selected_ids]\n","        # log\n","        sorted_new_ys, sorted_values = zip(*sorted(zip(new_ys, values), key=lambda x: x[1], reverse=True))\n","        if verbose:\n","            print(f'-- new_ys --: {sorted_new_ys}\\n-- sol values --: {sorted_values}\\n-- choices --: {select_new_ys}\\n')\n","\n","        infos.append({'step': step, 'x': x, 'ys': ys, 'new_ys': new_ys, 'values': values, 'select_new_ys': select_new_ys})\n","        ys = select_new_ys\n","\n","    return ys[0], infos\n","\n","def evaluate_dataset(pipe, data, dataset_name, verbose=False, n_generate_sample=6, n_select_sample=3, depth=3):\n","    \"\"\"Evaluate model on a specific dataset\"\"\"\n","    print(f\"Starting evaluation of {dataset_name} dataset...\")\n","\n","    results = []\n","    start_time = time.time()\n","\n","    for i, sample in enumerate(data):\n","        # Create appropriate prompt\n","        prompt = create_prompt(sample, dataset_name)\n","\n","        # Print the prompt being sent to the model\n","        print(f\"\\n{'='*80}\")\n","        print(f\"SAMPLE {i+1}/{len(data)} - DATASET: {dataset_name}\")\n","        print(f\"{'-'*80}\")\n","        print(f\"PROMPT:\")\n","        print(f\"{prompt}\")\n","        print(f\"{'-'*80}\")\n","\n","        # Run inference\n","        output, infos = solve(pipe, prompt, n_generate_sample=n_generate_sample, n_select_sample=n_select_sample, depth=depth, verbose=verbose)\n","\n","        # Print the model's response\n","        print(f\"MODEL RESPONSE:\")\n","        print(f\"{output}\")\n","\n","        # Extract prediction and ground truth\n","        prediction = extract_prediction(output, dataset_name, sample)\n","        true_label = get_ground_truth(sample, dataset_name)\n","\n","        # Evaluate correctness\n","        correct = evaluate_correctness(prediction, true_label, dataset_name)\n","\n","        print(f\"{'-'*80}\")\n","        print(f\"PREDICTION: {prediction}\")\n","        print(f\"TRUE LABEL: {true_label}\")\n","        print(f\"CORRECT: {correct}\")\n","        print(f\"{'='*80}\")\n","\n","        temp_time = time.time()\n","        elapsed = temp_time - start_time\n","        avg_time = elapsed / (i+1)\n","        # Store result\n","        result = {\n","            \"input\": prompt,\n","            \"output\": output,\n","            \"prediction\": prediction,\n","            \"true_label\": true_label,\n","            \"correct\": correct,\n","            \"infos\": infos,\n","            \"avg_time\": avg_time\n","        }\n","        results.append(result)\n","\n","        # Print progress\n","        if (i+1) % 10 == 0 or i == 0:\n","            elapsed = time.time() - start_time\n","            avg_time = elapsed / (i+1)\n","            remaining = avg_time * (len(data) - i - 1)\n","            print(f\"Processed {i+1}/{len(data)} samples - \"\n","                  f\"Avg time per sample: {avg_time:.2f}s - \"\n","                  f\"Estimated time remaining: {remaining/60:.1f} minutes\")\n","\n","\n","    # Calculate overall accuracy\n","    accuracy = sum(r['correct'] for r in results) / len(results)\n","\n","    print(f\"\\nResults for {dataset_name}:\")\n","    print(f\"Overall accuracy: {accuracy:.2f}\")\n","\n","    latency = results[-1]['avg_time']\n","    print(f\"Average latency: {latency:.2f}s\")\n","\n","    # Save results\n","    output_dir = f\"results_{dataset_name}\"\n","    os.makedirs(output_dir, exist_ok=True)\n","    # save to json\n","    with open(f\"{output_dir}/results_tot.json\", \"w\") as f:\n","        json.dump(results, f, indent=2)\n","\n","\n","    print(f\"Results saved to {output_dir}/\")\n","\n","    return accuracy, results\n","\n","\n","# Load model and tokenizer\n","print(\"Loading model and tokenizer...\")\n","model_path = \"meta-llama/Llama-3.2-1B-Instruct\"\n","# tokenizer = AutoTokenizer.from_pretrained(model_path)\n","# model = AutoModelForCausalLM.from_pretrained(model_path).to(\"cuda\")\n","pipe = pipeline(\"text-generation\", model=model_path, torch_dtype=torch.bfloat16, device_map=\"cuda:0\")\n","\n","# # Ensure the tokenizer has a pad_token\n","# if tokenizer.pad_token is None:\n","#     tokenizer.add_special_tokens({'pad_token': tokenizer.eos_token})\n","#     model.resize_token_embeddings(len(tokenizer))\n","\n","print(f\"Model loaded successfully\")\n","\n","# try a simple prompt\n","simple_prompt = \"What is the sum of 2 and 3?\"\n","simple_output = run_inference(pipe, simple_prompt)\n","print(f\"Simple prompt: {simple_prompt}\")\n","print(f\"Simple output: {simple_output}\")\n","\n","\n","data_dir = \"../data\"\n","\n","# Define dataset paths\n","datasets = {\n","    \"VitaminC\": f\"{data_dir}/VitaminC/test.jsonl\",\n","    \"2WikiMultihopQA\": f\"{data_dir}/2WikiMultihopQA/test.json\",\n","    \"Bamboogle\": f\"{data_dir}/Bamboogle/test.json\",\n","    \"FEVER\": f\"{data_dir}/FEVER/fever_test.jsonl\",\n","    \"FEVEROUS\": f\"{data_dir}/FEVEROUS/feverous_test.jsonl\",\n","    \"HotpotQA\": f\"{data_dir}/HotpotQA/test.json\",\n","    \"SVAMP\": f\"{data_dir}/SVAMP/test.json\"\n","}\n","\n","# Display available datasets\n","print(f\"Available datasets for evaluation:\")\n","for i, dataset_name in enumerate(datasets.keys()):\n","    print(f\"{i+1}. {dataset_name}\")\n","\n","datasets_to_evaluate = [\n","    # \"VitaminC\",\n","    # \"FEVER\",\n","    # \"FEVEROUS\",\n","    # \"HotpotQA\",\n","    # \"2WikiMultihopQA\",\n","    \"SVAMP\",\n","    # \"Bamboogle\"\n","]  # Evaluate all available datasets\n","# Initialize results summary\n","summary = {}\n","\n","# Process each selected dataset\n","for dataset_name in datasets_to_evaluate:\n","    if dataset_name not in datasets:\n","        print(f\"Dataset {dataset_name} not found in available datasets\")\n","        continue\n","\n","    # Load dataset\n","    file_path = datasets[dataset_name]\n","    data = load_dataset(file_path, dataset_name, limit=10)  # Limit to 150 samples\n","\n","    if data:\n","        # Evaluate on the dataset\n","        accuracy, _ = evaluate_dataset(pipe, data, dataset_name, verbose=False)\n","        summary[dataset_name] = accuracy\n","\n","# Print summary of results\n","print(f\"\\nSummary of Results:\")\n","for dataset, accuracy in summary.items():\n","    print(f\"{dataset}: {accuracy:.4f}\")\n"]},{"cell_type":"code","source":["def extract_fact_verification_prediction(output):\n","    \"\"\"Extract fact verification label from model output.\"\"\"\n","    output_lower = output.lower()\n","\n","    # 1) Check if there's an ANSWER: line with a direct label.\n","    answers = re.findall(r'ANSWER:\\s*(SUPPORTS|REFUTES|NOT ENOUGH INFO)', output, re.MULTILINE)\n","    if answers:\n","        return answers[-1]\n","\n","    # 2) Look for known patterns in the text.\n","    verdict_patterns = [\n","        r\"verdict\\s*:?\\s*(supports|refutes|not enough info)\",\n","        r\"therefore,?\\s+(the evidence)?\\s*(supports|refutes|provides not enough info)\",\n","        r\"(my conclusion|my answer) is\\s*:?\\s*(supports|refutes|not enough info)\",\n","        r\"the evidence (supports|refutes|provides not enough info)\",\n","        r\"(supports|refutes|not enough info) the claim\"\n","    ]\n","    for pattern in verdict_patterns:\n","        matches = re.findall(pattern, output_lower)\n","        if matches:\n","            label = matches[-1][-1] if isinstance(matches[-1], tuple) else matches[-1]\n","            return _normalize_fact_label(label)\n","\n","    # 3) Try searching the last quarter or entire text.\n","    last_quarter = output_lower[3*len(output_lower)//4:]\n","    return _search_label(last_quarter, output_lower)\n","\n","\n","def _normalize_fact_label(label):\n","    label_lower = label.lower()\n","    if \"not enough\" in label_lower:\n","        return \"NOT ENOUGH INFO\"\n","    if \"supports\" in label_lower:\n","        return \"SUPPORTS\"\n","    if \"refutes\" in label_lower:\n","        return \"REFUTES\"\n","    return \"NOT ENOUGH INFO\"\n","\n","\n","def _search_label(last_quarter, output):\n","    for label in [\"not enough info\", \"supports\", \"refutes\"]:\n","        if label in last_quarter or label in output:\n","            return _normalize_fact_label(label)\n","    return \"NOT ENOUGH INFO\"\n"],"metadata":{"id":"EEQQMTjPyRNP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import re\n","answer = \"\"\"\n","Step 1, To analyze the claim, let's break down the evidence provided by the British Phonographic Industry (BPI):\n","\n","- Westlife has been certified for 13 million albums.\n","- Westlife has been certified for 9.8 million singles.\n","- The total combined sales in the UK is more than 23 million.\n","\n","Step 1: Analyzing the albums certification\n","The BPI certification for albums is typically based on physical or digital album sales. However, the certification does not directly translate to the number of albums sold. It's possible that the certification is for a specific number of albums sold, not necessarily the total number of albums.\n","\n","Step 2, To determine if the evidence SUPPORTS, REFUTES, or provides NOT ENOUGH INFO for the claim, let's analyze the evidence step by step:\n","\n","Step 1: Analyzing the albums certification\n","The BPI certification for albums is typically based on physical or digital album sales. However, the certification does not directly translate to the number of albums sold. It's possible that the certification is for a specific number of albums sold, not necessarily the total number of albums.\n","\n","Step 2: Analyzing the singles certification\n","The BPI certification for singles is based on physical or digital single sales. This certification also does not directly translate to the number of singles sold. It's possible that the certification is for a specific number of singles sold, not necessarily the total number of singles.\n","\n","ANSWER: ANSWER: REFUTES\n","\"\"\"\n","extract_fact_verification_prediction(answer)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":36},"id":"EbOFJG5dyVr5","executionInfo":{"status":"ok","timestamp":1741505220671,"user_tz":480,"elapsed":12,"user":{"displayName":"Sihang Zeng","userId":"03734345999946873694"}},"outputId":"feb12324-ff3c-441a-80db-d36f30b50e82"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'REFUTES'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":9}]},{"cell_type":"code","source":[],"metadata":{"id":"_UtkguM5yqzw"},"execution_count":null,"outputs":[]}]}
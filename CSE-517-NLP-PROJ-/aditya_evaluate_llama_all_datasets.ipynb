{"cells":[{"cell_type":"markdown","metadata":{"id":"KMb2bCR-wKL5"},"source":["# Evaluating Llama-3.2-1B on Multiple Datasets\n","\n","This notebook evaluates the Llama-3.2-1B model on multiple datasets:\n","- **VitaminC**: Fact verification\n","- **FEVER/FEVEROUS**: Fact verification\n","- **HotpotQA/2WikiMultihopQA**: Multi-hop question answering\n","- **SVAMP**: Math word problems\n","- **Bamboogle**: General question answering\n","\n","The evaluation uses dataset-specific prompting strategies and metrics."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Uj5FEEgLwKL6"},"outputs":[],"source":["import json\n","import time\n","import re\n","import random\n","import os\n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","from google.colab import drive\n","from transformers import AutoModelForCausalLM, AutoTokenizer\n","from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n","import torch\n","import math\n","\n","# ANSI color codes for terminal output\n","BLUE = '\\033[94m'     # For sample information\n","GREEN = '\\033[92m'    # For correct predictions and success messages\n","RED = '\\033[91m'      # For incorrect predictions\n","YELLOW = '\\033[93m'   # For predictions and headers\n","CYAN = '\\033[96m'     # For progress information\n","PURPLE = '\\033[95m'   # For model responses\n","BOLD = '\\033[1m'      # Bold text\n","ENDC = '\\033[0m'      # End color\n","\n","# Mount Google Drive\n","drive.mount('/content/drive')"]},{"cell_type":"markdown","metadata":{"id":"6g-w27t0wKL7"},"source":["## Dataset Loading Functions\n","\n","Functions to load different dataset formats (JSON and JSONL)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"o7QssGFuwKL8"},"outputs":[],"source":["def load_dataset(file_path, dataset_name, limit=150):\n","    \"\"\"Load samples from various datasets with appropriate format handling\"\"\"\n","    print(f\"{CYAN}Loading {dataset_name} dataset from {file_path}...{ENDC}\")\n","    data = []\n","\n","    if not os.path.exists(file_path):\n","        print(f\"{RED}File not found: {file_path}{ENDC}\")\n","        return []\n","\n","    if file_path.endswith('.jsonl'):\n","        with open(file_path, 'r', encoding='utf-8') as f:\n","            for line in f:\n","                if line.strip():  # Skip empty lines\n","                    data.append(json.loads(line))\n","                    if len(data) >= limit:\n","                        break\n","    else:  # .json files\n","        with open(file_path, 'r', encoding='utf-8') as f:\n","            json_data = json.load(f)\n","            if isinstance(json_data, list):\n","                data = json_data[:limit]\n","            else:\n","                # Handle nested structures if needed\n","                if 'data' in json_data:\n","                    data = json_data['data'][:limit]\n","                else:\n","                    print(f\"{YELLOW}Warning: Unexpected JSON structure in {file_path}{ENDC}\")\n","                    data = [json_data]  # Just use the whole object as one sample\n","\n","    print(f\"{CYAN}Loaded {len(data)} samples from {dataset_name}.{ENDC}\")\n","    return data"]},{"cell_type":"markdown","metadata":{"id":"5Bl0sDs1wKL9"},"source":["## Prompt Creation Functions\n","\n","Different prompting strategies for each dataset type"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zHcvt2O5wKL9"},"outputs":[],"source":["def create_prompt(sample, dataset_name):\n","    \"\"\"Create appropriate prompts based on dataset type\"\"\"\n","\n","    if dataset_name == \"VitaminC\" or dataset_name == \"FEVER\" or dataset_name == \"FEVEROUS\":\n","        # Fact verification datasets\n","        if dataset_name == \"VitaminC\":\n","            claim = sample[\"claim\"]\n","            evidence = sample[\"evidence\"]\n","        elif dataset_name == \"FEVER\":\n","            claim = sample[\"claim\"]\n","            evidence = sample.get(\"evidence\", \"\") or sample.get(\"context\", \"\")\n","        elif dataset_name == \"FEVEROUS\":\n","            claim = sample[\"claim\"]\n","            evidence = sample.get(\"evidence\", \"\") or sample.get(\"context\", \"\")\n","\n","        return create_fact_verification_prompt(claim, evidence)\n","\n","    elif dataset_name == \"HotpotQA\" or dataset_name == \"2WikiMultihopQA\":\n","        # Multi-hop QA datasets\n","        question = sample.get(\"question\", \"\") or sample.get(\"query\", \"\")\n","        context = sample.get(\"context\", \"\")\n","        if not context and \"original_context\" in sample:\n","            context = sample[\"original_context\"]\n","\n","        return create_qa_prompt(question, context)\n","\n","    elif dataset_name == \"SVAMP\":\n","        # Math word problem dataset\n","        question = sample.get(\"question\", \"\") or sample.get(\"body\", \"\")\n","        return create_math_prompt(question)\n","\n","    elif dataset_name == \"Bamboogle\":\n","        # Bamboogle dataset - assume it's a QA task\n","        question = sample.get(\"question\", \"\")\n","        context = sample.get(\"context\", \"\")\n","\n","        if not question and \"answer\" in sample:\n","            # If no question is provided but there's an answer, create a generic prompt\n","            return f\"Please provide a detailed answer for this question.\\n\\nQuestion: What is the answer?\\n\\nAnswer:\"\n","\n","        return create_qa_prompt(question, context)\n","\n","    else:\n","        # Generic prompt for unknown datasets\n","        return f\"Please analyze this data and provide a detailed response:\\n\\n{json.dumps(sample, indent=2)}\"\n","\n","def create_fact_verification_prompt(claim, evidence):\n","    \"\"\"Create a chain of thought prompt for fact checking\"\"\"\n","    prompt = f\"\"\"You are a fact-checking AI. Determine if the evidence SUPPORTS, REFUTES, or provides NOT ENOUGH INFO for the claim.\n","\n","Claim: \"{claim}\"\n","Evidence: \"{evidence}\"\n","\n","Let's think through this step-by-step:\n","\n","1) First, I'll analyze what the claim is stating:\n","   - What is the main assertion?\n","   - Are there specific details, numbers, or comparisons?\n","\n","2) Next, I'll examine what the evidence contains:\n","   - What information is provided in the evidence?\n","   - Are there specific facts, figures, or statements?\n","\n","3) Now, I'll compare the claim with the evidence:\n","   - Does the evidence directly address the claim?\n","   - Is there alignment or contradiction between claim and evidence?\n","   - Is any critical information missing from the evidence needed to verify the claim?\n","\n","4) Finally, I'll determine my verdict with reasoning:\n","   - SUPPORTS: Evidence confirms the claim is true\n","   - REFUTES: Evidence contradicts the claim\n","   - NOT ENOUGH INFO: Evidence is insufficient to either support or refute the claim\n","\n","My detailed analysis:\"\"\"\n","    return prompt\n","\n","def create_qa_prompt(question, context):\n","    \"\"\"Create a prompt for question answering tasks\"\"\"\n","    prompt = f\"\"\"You are an AI assistant that helps with question answering. Read the context provided and answer the question.\n","\n","Context:\n","{context}\n","\n","Question: {question}\n","\n","Let's think step-by-step to find the answer based on the context:\n","\n","1) First, I'll identify what the question is asking for.\n","\n","2) Next, I'll search the context for relevant information related to the question.\n","\n","3) Then, I'll reason about the information to determine the answer.\n","\n","4) Finally, I'll provide a concise and direct answer to the question.\n","\n","My reasoning:\"\"\"\n","    return prompt\n","\n","def create_math_prompt(question):\n","    \"\"\"Create a prompt for math word problems\"\"\"\n","    prompt = f\"\"\"You are an AI assistant that helps solve math word problems. Solve the following problem step-by-step.\n","\n","Problem: {question}\n","\n","I'll solve this step-by-step:\n","\n","1) First, I'll identify the key information in the problem.\n","\n","2) Next, I'll determine which mathematical operations are needed.\n","\n","3) Then, I'll set up and solve the equations.\n","\n","4) Finally, I'll provide the numeric answer.\n","\n","My solution:\"\"\"\n","    return prompt"]},{"cell_type":"markdown","metadata":{"id":"EepJr9zzwKL-"},"source":["## Model Inference Function"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OHUMLmPCwKL_"},"outputs":[],"source":["def run_inference(model, tokenizer, prompt, device=\"cuda\"):\n","    \"\"\"Run inference on the model\"\"\"\n","    # Tokenize input with attention mask\n","    encoding = tokenizer(prompt, return_tensors=\"pt\", padding=True, truncation=True, max_length=128000)\n","\n","    inputs = {\n","        'input_ids': encoding.input_ids.to(device),\n","        'attention_mask': encoding.attention_mask.to(device)\n","    }\n","\n","    with torch.no_grad():\n","        outputs = model.generate(\n","            input_ids=inputs['input_ids'],\n","            attention_mask=inputs['attention_mask'],\n","            max_new_tokens=2048,  # Adjust based on desired output length\n","            temperature=0.2,      # Lower temperature for focused output\n","            do_sample=True,\n","        )\n","\n","    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n"]},{"cell_type":"markdown","metadata":{"id":"dNwVOK5cwKMA"},"source":["## Prediction Extraction Functions\n","\n","Functions to extract structured predictions from model outputs"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QcoTjldGwKMB"},"outputs":[],"source":["def extract_prediction(output, dataset_name, sample=None):\n","    \"\"\"Extract the prediction from the model's output based on dataset type\"\"\"\n","\n","    if dataset_name == \"VitaminC\" or dataset_name == \"FEVER\" or dataset_name == \"FEVEROUS\":\n","        # Fact verification prediction extraction\n","        return extract_fact_verification_prediction(output)\n","\n","    elif dataset_name == \"HotpotQA\" or dataset_name == \"2WikiMultihopQA\":\n","        # QA prediction extraction - extract the answer\n","        return extract_qa_prediction(output)\n","\n","    elif dataset_name == \"SVAMP\":\n","        # Math prediction - extract numeric answer\n","        return extract_math_prediction(output)\n","\n","    elif dataset_name == \"Bamboogle\":\n","        # Extract the answer for Bamboogle\n","        return extract_qa_prediction(output)\n","\n","    else:\n","        # Generic extraction for unknown datasets\n","        return output.strip()\n","\n","def extract_fact_verification_prediction(output):\n","    \"\"\"Extract fact verification label from model output\"\"\"\n","    output = output.lower()\n","\n","    # First check for a clear final verdict statement\n","    verdict_patterns = [\n","        r\"verdict\\s*:?\\s*(supports|refutes|not enough info)\",\n","        r\"therefore,?\\s+(the evidence)?\\s*(supports|refutes|provides not enough info)\",\n","        r\"(my conclusion|my answer) is\\s*:?\\s*(supports|refutes|not enough info)\",\n","        r\"the evidence (supports|refutes|provides not enough info)\",\n","        r\"(supports|refutes|not enough info) the claim\"\n","    ]\n","\n","    for pattern in verdict_patterns:\n","        matches = re.findall(pattern, output)\n","        if matches:\n","            last_match = matches[-1]\n","            if isinstance(last_match, tuple):\n","                last_match = last_match[-1]  # Get the last group\n","\n","            if \"not enough\" in last_match:\n","                return \"NOT ENOUGH INFO\"\n","            elif \"supports\" in last_match:\n","                return \"SUPPORTS\"\n","            elif \"refutes\" in last_match:\n","                return \"REFUTES\"\n","\n","    # If no clear verdict statement, look for the labels in the last quarter of text\n","    last_quarter = output[3*len(output)//4:]\n","\n","    if \"not enough info\" in last_quarter:\n","        return \"NOT ENOUGH INFO\"\n","    if \"supports\" in last_quarter:\n","        return \"SUPPORTS\"\n","    if \"refutes\" in last_quarter:\n","        return \"REFUTES\"\n","\n","    # If still no verdict found, check the full text\n","    if \"not enough info\" in output:\n","        return \"NOT ENOUGH INFO\"\n","    if \"supports\" in output:\n","        return \"SUPPORTS\"\n","    if \"refutes\" in output:\n","        return \"REFUTES\"\n","\n","    # Default\n","    return \"NOT ENOUGH INFO\"\n","\n","def extract_qa_prediction(output):\n","    \"\"\"Extract the answer from QA model output\"\"\"\n","    # Look for patterns like \"Answer: X\", \"The answer is X\", etc.\n","    answer_patterns = [\n","        r\"(?:answer|the answer)(?:\\s+is)?(?:\\s*:)?\\s*(?:is)?\\s*(.*?)(?:$|\\.|\\n)\",\n","        r\"(?:therefore|thus|so|hence),?\\s*(?:the answer|answer)(?:\\s+is)?(?:\\s*:)?\\s*(.*?)(?:$|\\.|\\n)\",\n","        r\"(?:final answer|in conclusion)(?:\\s*:)?\\s*(.*?)(?:$|\\.|\\n)\"\n","    ]\n","\n","    for pattern in answer_patterns:\n","        matches = re.search(pattern, output.lower())\n","        if matches:\n","            return matches.group(1).strip()\n","\n","    # If no pattern matches, return the last non-empty line as a fallback\n","    lines = [line.strip() for line in output.split('\\n') if line.strip()]\n","    if lines:\n","        return lines[-1]\n","\n","    return output.strip()\n","\n","def extract_math_prediction(output):\n","    \"\"\"Extract numeric answer from math problem solution\"\"\"\n","    # Look for numbers in the last lines of the output, focusing on final answer patterns\n","    answer_patterns = [\n","        r\"(?:answer|the answer|result|the result)(?:\\s+is)?(?:\\s*:)?\\s*(-?[\\d,]+\\.?\\d*)\",\n","        r\"(?:=|equals)\\s*(-?[\\d,]+\\.?\\d*)\",\n","        r\"(?:therefore|thus|so|hence),?\\s*(?:the answer|answer)(?:\\s+is)?(?:\\s*:)?\\s*(-?[\\d,]+\\.?\\d*)\",\n","        r\"(?:final answer|in conclusion)(?:\\s*:)?\\s*(-?[\\d,]+\\.?\\d*)\"\n","    ]\n","\n","    for pattern in answer_patterns:\n","        matches = re.search(pattern, output.lower())\n","        if matches:\n","            # Clean the number format (remove commas, etc.)\n","            num_str = matches.group(1).replace(',', '')\n","            try:\n","                return float(num_str)\n","            except ValueError:\n","                pass\n","\n","    # Fall back to looking for any number in the last few lines\n","    lines = output.split('\\n')[-5:]  # Look at last 5 lines\n","    for line in reversed(lines):\n","        nums = re.findall(r\"(-?[\\d,]+\\.?\\d*)\", line)\n","        for num in nums:\n","            try:\n","                return float(num.replace(',', ''))\n","            except ValueError:\n","                pass\n","\n","    return output.strip()"]},{"cell_type":"markdown","metadata":{"id":"mvyD-l76wKMC"},"source":["## Evaluation Metrics Functions\n","\n","Functions to compare predictions with ground truth"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NJPBd4qdwKMC"},"outputs":[],"source":["def evaluate_correctness(prediction, ground_truth, dataset_name):\n","    \"\"\"Evaluate if the prediction is correct based on dataset type\"\"\"\n","\n","    if dataset_name == \"VitaminC\" or dataset_name == \"FEVER\" or dataset_name == \"FEVEROUS\":\n","        # Fact verification - direct comparison\n","        return prediction == ground_truth\n","\n","    elif dataset_name == \"HotpotQA\" or dataset_name == \"2WikiMultihopQA\" or dataset_name == \"Bamboogle\":\n","        # QA evaluation - normalize and compare\n","        return normalize_qa_answers(prediction, ground_truth)\n","\n","    elif dataset_name == \"SVAMP\":\n","        # Math evaluation - numeric comparison\n","        return evaluate_math_correctness(prediction, ground_truth)\n","\n","    else:\n","        # Generic comparison for unknown datasets\n","        return prediction == ground_truth\n","\n","def normalize_qa_answers(prediction, ground_truth):\n","    \"\"\"Normalize and compare QA answers with flexible matching\"\"\"\n","    if not prediction or not ground_truth:\n","        return False\n","\n","    # Handle list or dictionary ground truths\n","    if isinstance(ground_truth, list):\n","        ground_truth = \" \".join([str(item) for item in ground_truth])\n","    elif isinstance(ground_truth, dict):\n","        if \"answer\" in ground_truth:\n","            ground_truth = ground_truth[\"answer\"]\n","        else:\n","            ground_truth = str(ground_truth)\n","\n","    # Normalize both strings\n","    pred_norm = prediction.lower().strip()\n","    truth_norm = str(ground_truth).lower().strip()\n","\n","    # Remove punctuation and extra spaces\n","    pred_norm = re.sub(r'[^\\w\\s]', '', pred_norm).strip()\n","    truth_norm = re.sub(r'[^\\w\\s]', '', truth_norm).strip()\n","\n","    # Check if prediction contains ground truth or vice versa\n","    return pred_norm in truth_norm or truth_norm in pred_norm\n","\n","def evaluate_math_correctness(prediction, ground_truth):\n","    \"\"\"Evaluate correctness of math answers with tolerance\"\"\"\n","    try:\n","        # Convert to numeric values\n","        if isinstance(prediction, str):\n","            prediction = float(re.search(r'(-?[\\d.]+)', prediction.replace(',', '')).group(1))\n","\n","        if isinstance(ground_truth, str):\n","            ground_truth = float(re.search(r'(-?[\\d.]+)', ground_truth.replace(',', '')).group(1))\n","\n","        # Compare with tolerance\n","        tolerance = 0.01\n","        return abs(float(prediction) - float(ground_truth)) < tolerance\n","    except (ValueError, TypeError, AttributeError):\n","        return False\n","\n","def get_ground_truth(sample, dataset_name):\n","    \"\"\"Extract ground truth from sample based on dataset type\"\"\"\n","\n","    if dataset_name == \"VitaminC\":\n","        return sample.get(\"label\", \"\")\n","\n","    elif dataset_name == \"FEVER\" or dataset_name == \"FEVEROUS\":\n","        return sample.get(\"label\", \"\")\n","\n","    elif dataset_name == \"HotpotQA\" or dataset_name == \"2WikiMultihopQA\":\n","        return sample.get(\"answer\", \"\")\n","\n","    elif dataset_name == \"SVAMP\":\n","        return sample.get(\"answer\", None)\n","\n","    elif dataset_name == \"Bamboogle\":\n","        return sample.get(\"answer\", \"\")\n","\n","    else:\n","        return None"]},{"cell_type":"markdown","metadata":{"id":"bNibqi0cwKMD"},"source":["## Visualization Functions\n","\n","Functions to create visualizations of results"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fF89g18DwKMD"},"outputs":[],"source":["def visualize_results(results_df, dataset_name):\n","    \"\"\"Create visualizations of results for a specific dataset\"\"\"\n","    output_dir = f\"results_{dataset_name}\"\n","    os.makedirs(output_dir, exist_ok=True)\n","\n","    if 'true_label' in results_df.columns and 'prediction' in results_df.columns:\n","        # Fact verification datasets\n","        if len(results_df) > 0:\n","            try:\n","                labels = sorted(list(set(results_df['true_label'].unique()).union(set(results_df['prediction'].unique()))))\n","\n","                # Create confusion matrix\n","                cm = confusion_matrix(\n","                    results_df['true_label'],\n","                    results_df['prediction'],\n","                    labels=labels\n","                )\n","\n","                plt.figure(figsize=(10, 8))\n","                sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=labels, yticklabels=labels)\n","                plt.xlabel('Predicted')\n","                plt.ylabel('True')\n","                plt.title(f'Confusion Matrix - {dataset_name}')\n","                plt.tight_layout()\n","                plt.savefig(f\"{output_dir}/confusion_matrix.png\")\n","                plt.close()\n","\n","                # Class-wise accuracy\n","                class_accuracy = {}\n","                for cls in labels:\n","                    cls_indices = results_df['true_label'] == cls\n","                    if cls_indices.any():\n","                        correct = (results_df.loc[cls_indices, 'prediction'] == cls).sum()\n","                        class_accuracy[cls] = correct / cls_indices.sum()\n","\n","                plt.figure(figsize=(10, 6))\n","                sns.barplot(x=list(class_accuracy.keys()), y=list(class_accuracy.values()))\n","                plt.ylim(0, 1)\n","                plt.title(f'Class-wise Accuracy - {dataset_name}')\n","                plt.ylabel('Accuracy')\n","                plt.tight_layout()\n","                plt.savefig(f\"{output_dir}/class_accuracy.png\")\n","                plt.close()\n","            except Exception as e:\n","                print(f\"{RED}Error creating visualizations for {dataset_name}: {e}{ENDC}\")\n","\n","    # Overall accuracy\n","    plt.figure(figsize=(6, 4))\n","    accuracy = results_df['correct'].mean()\n","    plt.bar(['Accuracy'], [accuracy])\n","    plt.ylim(0, 1)\n","    plt.title(f'Overall Accuracy - {dataset_name}')\n","    plt.tight_layout()\n","    plt.savefig(f\"{output_dir}/overall_accuracy.png\")\n","    plt.close()\n","\n","    # Save sample correct/incorrect examples\n","    correct_samples = results_df[results_df['correct']].head(3)\n","    incorrect_samples = results_df[~results_df['correct']].head(3)\n","\n","    with open(f\"{output_dir}/sample_results.txt\", 'w') as f:\n","        f.write(f\"=== Sample Correct Examples ({dataset_name}) ===\\n\\n\")\n","        for _, sample in correct_samples.iterrows():\n","            f.write(f\"Input: {sample.get('input', '')}\\n\")\n","            f.write(f\"True: {sample.get('true_label', '')}\\n\")\n","            f.write(f\"Prediction: {sample.get('prediction', '')}\\n\")\n","            f.write(\"\\n---\\n\\n\")\n","\n","        f.write(f\"=== Sample Incorrect Examples ({dataset_name}) ===\\n\\n\")\n","        for _, sample in incorrect_samples.iterrows():\n","            f.write(f\"Input: {sample.get('input', '')}\\n\")\n","            f.write(f\"True: {sample.get('true_label', '')}\\n\")\n","            f.write(f\"Prediction: {sample.get('prediction', '')}\\n\")\n","            f.write(\"\\n---\\n\\n\")"]},{"cell_type":"markdown","metadata":{"id":"qA6rfe8WwKME"},"source":["## Dataset Evaluation Function\n","\n","Function to evaluate model on each dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"o3iCGT9NwKME"},"outputs":[],"source":["def evaluate_dataset(model, tokenizer, data, dataset_name):\n","    \"\"\"Evaluate model on a specific dataset\"\"\"\n","    print(f\"{BOLD}{CYAN}Starting evaluation of {dataset_name} dataset...{ENDC}\")\n","\n","    results = []\n","    start_time = time.time()\n","\n","    for i, sample in enumerate(data):\n","        # Create appropriate prompt\n","        prompt = create_prompt(sample, dataset_name)\n","\n","        # Print the prompt being sent to the model\n","        print(f\"\\n{BOLD}{'='*80}{ENDC}\")\n","        print(f\"{BLUE}{BOLD}SAMPLE {i+1}/{len(data)} - DATASET: {dataset_name}{ENDC}\")\n","        print(f\"{BLUE}{'-'*80}{ENDC}\")\n","        print(f\"{GREEN}PROMPT:{ENDC}\")\n","        print(f\"{GREEN}{prompt}{ENDC}\")\n","        print(f\"{BLUE}{'-'*80}{ENDC}\")\n","\n","        # Run inference\n","        output = run_inference(model, tokenizer, prompt)\n","\n","        # Print the model's response\n","        print(f\"{PURPLE}MODEL RESPONSE:{ENDC}\")\n","        print(f\"{PURPLE}{output}{ENDC}\")\n","\n","        # Extract prediction and ground truth\n","        prediction = extract_prediction(output, dataset_name, sample)\n","        true_label = get_ground_truth(sample, dataset_name)\n","\n","        # Evaluate correctness\n","        correct = evaluate_correctness(prediction, true_label, dataset_name)\n","        correct_color = GREEN if correct else RED\n","\n","        print(f\"{BLUE}{'-'*80}{ENDC}\")\n","        print(f\"{YELLOW}PREDICTION: {prediction}{ENDC}\")\n","        print(f\"{YELLOW}TRUE LABEL: {true_label}{ENDC}\")\n","        print(f\"{correct_color}CORRECT: {correct}{ENDC}\")\n","        print(f\"{BOLD}{'='*80}{ENDC}\")\n","\n","        # Store result\n","        result = {\n","            \"input\": prompt,\n","            \"output\": output,\n","            \"prediction\": prediction,\n","            \"true_label\": true_label,\n","            \"correct\": correct\n","        }\n","        results.append(result)\n","\n","        # Print progress\n","        if (i+1) % 10 == 0 or i == 0:\n","            elapsed = time.time() - start_time\n","            avg_time = elapsed / (i+1)\n","            remaining = avg_time * (len(data) - i - 1)\n","            print(f\"{CYAN}Processed {i+1}/{len(data)} samples - \"\n","                  f\"Avg time per sample: {avg_time:.2f}s - \"\n","                  f\"Estimated time remaining: {remaining/60:.1f} minutes{ENDC}\")\n","\n","    # Calculate overall accuracy\n","    results_df = pd.DataFrame(results)\n","    accuracy = results_df['correct'].mean()\n","\n","    print(f\"\\n{BOLD}Results for {dataset_name}:{ENDC}\")\n","    print(f\"{BOLD}Overall accuracy: {accuracy:.2f}{ENDC}\")\n","\n","    # Save results\n","    output_dir = f\"results_{dataset_name}\"\n","    os.makedirs(output_dir, exist_ok=True)\n","    results_df.to_csv(f\"{output_dir}/results.csv\", index=False)\n","\n","    # Create visualizations\n","    visualize_results(results_df, dataset_name)\n","\n","    print(f\"{GREEN}Results saved to {output_dir}/{ENDC}\")\n","\n","    return accuracy, results_df"]},{"cell_type":"markdown","metadata":{"id":"NI0LMs8nwKMF"},"source":["## Model Loading and Dataset Evaluation\n","\n","Load the model and evaluate it on all datasets"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"P1y3cj8XwKMF"},"outputs":[],"source":["# Load model and tokenizer\n","print(\"Loading model and tokenizer...\")\n","model_path = \"/content/drive/Shareddrives/517 nlp project/Models/Llama-3.2-1B-Instruct\"\n","tokenizer = AutoTokenizer.from_pretrained(model_path)\n","model = AutoModelForCausalLM.from_pretrained(model_path).to(\"cuda\")\n","\n","# Ensure the tokenizer has a pad_token\n","if tokenizer.pad_token is None:\n","    tokenizer.add_special_tokens({'pad_token': tokenizer.eos_token})\n","    model.resize_token_embeddings(len(tokenizer))\n","\n","print(f\"{GREEN}Model loaded successfully{ENDC}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UEdEZrAXwKMF"},"outputs":[],"source":["# Define dataset paths\n","datasets = {\n","    \"VitaminC\": \"/content/drive/Shareddrives/517 nlp project/data/VitaminC/vitaminc/test.jsonl\",\n","    \"2WikiMultihopQA\": \"/content/drive/Shareddrives/517 nlp project/data/2WikiMultihopQA/test.json\",\n","    \"Bamboogle\": \"/content/drive/Shareddrives/517 nlp project/data/Bamboogle/test.json\",\n","    \"FEVER\": \"/content/drive/Shareddrives/517 nlp project/data/FEVER/fever_test.jsonl\",\n","    \"FEVEROUS\": \"/content/drive/Shareddrives/517 nlp project/data/FEVEROUS/feverous_test.jsonl\",\n","    \"HotpotQA\": \"/content/drive/Shareddrives/517 nlp project/data/HotpotQA/test.json\",\n","    \"SVAMP\": \"/content/drive/Shareddrives/517 nlp project/data/SVAMP/test.json\"\n","}\n","\n","# Display available datasets\n","print(f\"{CYAN}Available datasets for evaluation:{ENDC}\")\n","for i, dataset_name in enumerate(datasets.keys()):\n","    print(f\"{i+1}. {dataset_name}\")"]},{"cell_type":"markdown","metadata":{"id":"-BUpdcOgwKMF"},"source":["Test a single sample for all the datasets to ensure everything is working first"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"knJm1CFBwKMG"},"outputs":[],"source":["# Choose which datasets to evaluate\n","datasets_to_evaluate = [\n","    \"VitaminC\",\n","    \"FEVER\",\n","    \"FEVEROUS\",\n","    \"HotpotQA\",\n","    \"2WikiMultihopQA\",\n","    \"SVAMP\",\n","    \"Bamboogle\"\n","]  # Evaluate all available datasets\n","\n","# Initialize results summary\n","summary = {}\n","\n","# Process each selected dataset\n","for dataset_name in datasets_to_evaluate:\n","    if dataset_name not in datasets:\n","        print(f\"{RED}Dataset {dataset_name} not found in available datasets{ENDC}\")\n","        continue\n","\n","    # Load dataset\n","    file_path = datasets[dataset_name]\n","    data = load_dataset(file_path, dataset_name, limit=1)  # Limit to 1 sample\n","\n","    if data:\n","        # Evaluate on the dataset\n","        accuracy, _ = evaluate_dataset(model, tokenizer, data, dataset_name)\n","        summary[dataset_name] = accuracy"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"41nE9FMWwKMG"},"outputs":[],"source":["# Choose which datasets to evaluate\n","# You can modify this list to evaluate specific datasets\n","# datasets_to_evaluate = [\"VitaminC\", \"FEVER\"]  # Uncomment to evaluate specific datasets\n","datasets_to_evaluate = [\n","    \"VitaminC\",\n","    # \"FEVER\",\n","    # \"FEVEROUS\",\n","    # \"HotpotQA\",\n","    # \"2WikiMultihopQA\",\n","    # \"SVAMP\",\n","    # \"Bamboogle\"\n","]  # Evaluate all available datasets\n","# Initialize results summary\n","summary = {}\n","\n","# Process each selected dataset\n","for dataset_name in datasets_to_evaluate:\n","    if dataset_name not in datasets:\n","        print(f\"{RED}Dataset {dataset_name} not found in available datasets{ENDC}\")\n","        continue\n","\n","    # Load dataset\n","    file_path = datasets[dataset_name]\n","    data = load_dataset(file_path, dataset_name, limit=150)  # Limit to 150 samples\n","\n","    if data:\n","        # Evaluate on the dataset\n","        accuracy, _ = evaluate_dataset(model, tokenizer, data, dataset_name)\n","        summary[dataset_name] = accuracy"]},{"cell_type":"markdown","metadata":{"id":"oTe7iU_zwKMG"},"source":["## Results Visualization\n","\n","Create summary visualizations of model performance across datasets"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7e2XQQtZwKMG"},"outputs":[],"source":["# Print summary of results\n","print(f\"\\n{BOLD}{CYAN}Summary of Results:{ENDC}\")\n","for dataset, accuracy in summary.items():\n","    print(f\"{dataset}: {accuracy:.4f}\")\n","\n","# Create summary visualization\n","if summary:\n","    plt.figure(figsize=(12, 6))\n","    datasets = list(summary.keys())\n","    accuracies = list(summary.values())\n","\n","    # Sort by accuracy\n","    sorted_indices = np.argsort(accuracies)[::-1]\n","    sorted_datasets = [datasets[i] for i in sorted_indices]\n","    sorted_accuracies = [accuracies[i] for i in sorted_indices]\n","\n","    bars = plt.bar(sorted_datasets, sorted_accuracies)\n","\n","    # Add value labels on top of bars\n","    for bar in bars:\n","        height = bar.get_height()\n","        plt.text(bar.get_x() + bar.get_width()/2., height + 0.01,\n","                f'{height:.4f}', ha='center', va='bottom')\n","\n","    plt.ylim(0, 1.1)  # Set y limit to 0-1 with a small margin\n","    plt.xlabel('Datasets')\n","    plt.ylabel('Accuracy')\n","    plt.title('Llama-3.2-1B Performance Across Datasets')\n","    plt.xticks(rotation=45)\n","    plt.tight_layout()\n","    plt.savefig(\"summary_results.png\")\n","    plt.show()\n","\n","    # Save summary to CSV\n","    summary_df = pd.DataFrame(list(summary.items()), columns=['Dataset', 'Accuracy'])\n","    summary_df = summary_df.sort_values('Accuracy', ascending=False)\n","    summary_df.to_csv(\"summary_results.csv\", index=False)\n","\n","    print(f\"\\n{GREEN}Summary saved to summary_results.csv and summary_results.png{ENDC}\")\n","else:\n","    print(f\"\\n{RED}No results to visualize{ENDC}\")"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}